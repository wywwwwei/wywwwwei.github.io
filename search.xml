<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Database Outline</title>
    <url>/2021/05/14/database-outline/</url>
    <content><![CDATA[<h1>Database Outline</h1>
<h2 id="Disk-Manager">Disk Manager</h2>
<blockquote>
<p>Responsibilities:</p>
<ol>
<li>represents the database in files on disk</li>
<li>manages its memory and move data back-and-forth from disk</li>
</ol>
</blockquote>
<img src="image-20210420164301183.png" alt="image-20210420164301183" style="zoom:67%;" />
<ol>
<li>
<p>How the DBMS <strong>represents the database in files on disk</strong></p>
<ul>
<li>
<p>File storage</p>
<ul>
<li>
<p>n-array storage model (row store, apply for OLTP)</p>
<ul>
<li>
<p>advantage:</p>
<p>fast inserts, updates, and deletes</p>
<p>good for queries that need the entire tuple</p>
</li>
<li>
<p>disadvantage:</p>
<p>Not good for scanning large portions of the table and/or a subset of the attributes</p>
</li>
</ul>
</li>
<li>
<p>decomposition storage model (column store, apply for OLAP)</p>
<blockquote>
<p>Tuple identification</p>
<ol>
<li>
<p>Fixed-length offset</p>
<p>Each value is the same length for an attribute</p>
</li>
<li>
<p>Embedded tuple ids</p>
<p>Each value is stored with its tuple id in a column</p>
</li>
</ol>
</blockquote>
<ul>
<li>
<p>advantage:</p>
<p>reduces the amount wasted I/O because the DBMS only <strong>reads the data that it needs</strong></p>
<p>better query processing and data compression</p>
</li>
<li>
<p>disadvantage:</p>
<p>slow for point queries, inserts, updates, and deletes because of tuple splitting/stitching</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Page layout</p>
<ul>
<li>
<p>Slotted pages</p>
</li>
<li>
<p>Log-structed file organization</p>
<p>Log-structed compaction (level compaction or universe compaction)</p>
</li>
</ul>
</li>
<li>
<p>Tuple layout</p>
<ul>
<li>
<p>Data representation</p>
<table>
<thead>
<tr>
<th style="text-align:left">Type</th>
<th style="text-align:left">Representation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">INTEGER/BIGINT/SMALLINT/TINYINT</td>
<td style="text-align:left">C/C++ Representation</td>
</tr>
<tr>
<td style="text-align:left">FLOAT/REAL vs. NUMERIC/DECIMAL</td>
<td style="text-align:left">IEEE-754 Standard / Fixed-point Decimals</td>
</tr>
<tr>
<td style="text-align:left">VARCHAR/VARBINARY/TEXT/BLOB</td>
<td style="text-align:left">Header with length, followed by data bytes</td>
</tr>
<tr>
<td style="text-align:left">TIME/DATE/TIMESTAMP</td>
<td style="text-align:left">32/64-bit integer of (micro)seconds since Unix epoch</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>External value storage -&gt; BLOB</p>
<p>The DBMS <strong>cannot</strong> manipulate the contents of an external file because of without <strong>durability &amp; transaction protections</strong></p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>How the DBMS manages its <strong>memory</strong> and move data <strong>back-and-forth from disk</strong></p>
<ul>
<li>
<p>Spatial Control: (where)</p>
<p>Goal: keep pages that are used together often as physically close together as possible on disk</p>
</li>
<li>
<p>Temporal Control: (when)</p>
<p>Goal: minimize the number of stalls from having to read data from disk.</p>
</li>
</ul>
</li>
</ol>
<h2 id="Buffer-Pool-Manager">Buffer Pool Manager</h2>
<p>Memory region organized as an <strong>array of fixed-size pages</strong>.</p>
<p>An array entry is called a <strong>frame</strong>.</p>
<p>The <strong>page table</strong> keeps track of pages that are currently in memory and also maintains additional meta-data per page.</p>
<img src="image-20210420155458047.png" alt="image-20210420155458047" style="zoom:50%;" />
<ol>
<li>
<p>Allocation policies</p>
<ul>
<li>
<p>Global policies</p>
<p>Make decisions for all active txns.</p>
</li>
<li>
<p>Local policies</p>
<p>Allocate frames to a specific txn without considering the behavior of concurrent txns. Still need to support sharing pages.</p>
</li>
</ul>
</li>
<li>
<p>Optimization</p>
<ul>
<li>
<p>multiple buffer pools</p>
<p>Helps reduce latch contention and improve locality. Use <strong>embed object id</strong> or <strong>hashing</strong> for mapping.</p>
</li>
<li>
<p>pre-fetching</p>
</li>
<li>
<p>scan sharing</p>
</li>
<li>
<p>buffer pool bypass</p>
<p>The sequential scan operator will not store fetched pages in the buffer pool to avoid overhead.</p>
</li>
</ul>
</li>
<li>
<p>Buffer <strong>replacement polices</strong></p>
<ul>
<li>Least-Recently-Used</li>
<li>Clock</li>
</ul>
<p>LRU and CLOCK replacement policies are susceptible to sequential flooding.</p>
<blockquote>
<p>sequential flooding</p>
<p>A query performs a sequential scan that reads every page and this pollutes the buffer pool with pages that are read once and then never again.</p>
</blockquote>
<ul>
<li>LRU-K</li>
<li>Localization</li>
<li>Priority hints</li>
</ul>
</li>
</ol>
<h2 id="Access-Methods">Access Methods</h2>
<blockquote>
<p>Responsibilities:</p>
<ol>
<li>Data Organization (in memory / pages)</li>
<li>Concurrency</li>
</ol>
</blockquote>
<img src="image-20210505021551150.png" alt="image-20210505021551150" style="zoom:50%;" />
<ul>
<li>
<p>Data structures designed for <strong>Disk-oriented</strong> database</p>
<ol>
<li>hash table
<ul>
<li>hash function (how to map)
<ul>
<li>CRC-64</li>
<li>Google CityHash</li>
<li>…</li>
</ul>
</li>
<li>hashing schema (how to handle key collisions)
<ul>
<li>static hashing schema
<ul>
<li>Linear probe hashing</li>
<li>Robin hood hashing</li>
<li>Cuckoo hashing</li>
</ul>
</li>
</ul>
</li>
<li>dynamic hashing schema
<ul>
<li>chained hashing</li>
<li>extendible hashing</li>
<li>linear hashing</li>
</ul>
</li>
</ul>
</li>
<li>B+Tree</li>
</ol>
<ul>
<li>
<p>leaf node values</p>
<ul>
<li>Record Ids -&gt; A pointer to the location of the tuple that the index entry corresponds to.</li>
<li>Tuple Data -&gt; The actual contents of the tuple is stored in the leaf node.</li>
</ul>
</li>
<li>
<p>duplicate keys</p>
<ul>
<li>Append record id</li>
<li>Overflow leaf nodes</li>
</ul>
</li>
<li>
<p>node size</p>
<p>The slower the storage device, the larger the optimal node size for a B+Tree.</p>
</li>
<li>
<p>merge threshold</p>
<p>Delaying a merge operation may reduce the amount of reorganization.</p>
</li>
<li>
<p>variable length keys</p>
<ul>
<li>
<p>pointers</p>
</li>
<li>
<p>variable length nodes (requires careful memory management)</p>
</li>
<li>
<p>padding</p>
</li>
<li>
<p>key map / indirection</p>
<p>Embed an array of pointers that map to the key + value list within the node.</p>
</li>
</ul>
</li>
<li>
<p>intra-node search</p>
<ul>
<li>
<p>linear</p>
</li>
<li>
<p>binary</p>
</li>
<li>
<p>interpolation</p>
<p>Approximate location of desired key based on known distribution of key.</p>
</li>
</ul>
</li>
<li>
<p>optimization</p>
<ul>
<li>
<p>prefix compression</p>
<p>Instead of storing the entire key each time, extract common prefix and store only unique suffix for each key.</p>
</li>
<li>
<p>deduplication</p>
<p>The leaf node can store the key once and then a maintain a list of record ids with that key.</p>
</li>
<li>
<p>suffix truncation</p>
<p>Store a minimum prefix that is needed to correctly route probes into the index.</p>
</li>
<li>
<p>bulk insert</p>
<p>to build a new B+Tree for an existing table, first sort the keys and then build the index from the bottom up.</p>
</li>
<li>
<p>pointer swizzling</p>
<p>If a page is pinned in the buffer pool, then we can store raw pointers instead of page ids.</p>
</li>
</ul>
</li>
</ul>
<ol>
<li>
<p>Trie tree</p>
<ul>
<li>
<p>Trie key span</p>
<p>The span of a trie level is the number of bits that each partial key / digit represents.</p>
</li>
</ul>
</li>
<li>
<p>Radix tree</p>
</li>
<li>
<p>Trie variant</p>
<ul>
<li>Judy Arrays (HP)
<ul>
<li>Linear Node</li>
<li>Bitmap Node</li>
<li>Uncompressed Node</li>
</ul>
</li>
<li>ART Index (HyPer)</li>
<li>Masstree (Silo)</li>
</ul>
</li>
</ol>
</li>
</ul>
<img src="image-20210505021640552.png" alt="image-20210505021640552" style="zoom:50%;" />
<ul>
<li>
<p>Data structure designed for <strong>in-memory</strong> databases</p>
<ol>
<li>
<p>T Trees</p>
<p>Based on AVL Trees. Instead of storing keys in nodes, <strong>store pointers to their original values</strong>.</p>
<p>Advantages:</p>
<ul>
<li>Uses less memory because it does not store keys inside of each node.</li>
<li>The DBMS evaluates all predicates on a table at the same time when accessing a tuple (i.e., not just the predicates on indexed attributes).</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Difficult to rebalance.</li>
<li>Difficult to implement safe concurrent access.</li>
<li>Must chase pointers when scanning range or performing binary search inside of a node. (greatly hurts cache locality)</li>
</ul>
</li>
<li>
<p>BW Tree</p>
<p>Latch-free B+Tree index built for the Microsoft Hekaton project.</p>
<ul>
<li>garbage collection</li>
<li>Optimization (CMU open BW Tree)
<ul>
<li>Pre-Allocated Delta Records</li>
<li>Mapping Table Expansion</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li>
<p>Index types</p>
<ul>
<li>
<p>Partial index <a href="https://www.postgresql.org/docs/12/indexes-partial.html">PostgreSQL: Documentation: 12: 11.8. Partial Indexes</a></p>
</li>
<li>
<p>Covering index <a href="https://www.postgresql.org/docs/11/indexes-index-only-scans.html">PostgreSQL: Documentation: 11: 11.9. Index-Only Scans and Covering Indexes</a></p>
</li>
<li>
<p>index include column</p>
</li>
<li>
<p>functional / expression index <a href="https://www.postgresql.org/docs/12/indexes-expressional.html">PostgreSQL: Documentation: 12: 11.7. Indexes on Expressions</a></p>
</li>
<li>
<p>Inverted index -&gt; good at keyword searches</p>
<p>An inverted index stores a mapping of words to records that contain those words in the target attribute.</p>
</li>
</ul>
</li>
<li>
<p>Index concurrency control</p>
<blockquote>
<p>A protocol’s correctness criteria can vary:</p>
<ol>
<li>Logical Correctness: can a thread see the data that it is supposed to see</li>
<li>Physical Correctness: is the internal representation of the object found</li>
</ol>
</blockquote>
</li>
</ul>
<img src="image-20210421013209998.png" alt="image-20210421013209998" style="zoom:50%;" />
<ul>
<li>
<p>Latch implement</p>
<ul>
<li>Blocking OS mutex</li>
<li>Test-and-Set Spin Latch (TAS)</li>
<li>Reader-Writer Locks</li>
<li>Adaptive Spinlock</li>
<li>Queue-based Spinlock (MCS)</li>
</ul>
</li>
<li>
<p>Hash table latching</p>
<ul>
<li>Page latches</li>
<li>Slot latches</li>
</ul>
</li>
<li>
<p>B+ tree latching</p>
<ul>
<li>
<p>latch crabbing / coupling</p>
<p>Basic Idea:</p>
<ol>
<li>Get latch for parent</li>
<li>Get latch for child</li>
<li>Release latch for parent if “safe”. (A safe node is one that will not split or merge when updated.)</li>
</ol>
</li>
<li>
<p>better latching algorithm</p>
<p>Instead of assuming that there will be a split/merge, optimistically traverse the tree using read latches. If you guess wrong, repeat traversal with the pessimistic algorithm.</p>
</li>
<li>
<p>B link-Tree Optimization</p>
<p>When a leaf node overflows, delay updating its parent node.</p>
</li>
<li>
<p>versioned latch coupling</p>
<p>Optimistic crabbing scheme where writers are not blocked on readers.</p>
</li>
</ul>
</li>
</ul>
<h2 id="Operator-Execution">Operator Execution</h2>
<ol>
<li>
<p>Operator Algorithms</p>
<ul>
<li>
<p>Sort</p>
<ul>
<li>
<p>External merge sort</p>
<p>Pass #0: Use B buffer pages and Produce ⌈N / B⌉ sorted runs of size B</p>
<p>Pass #1,2,3…: Merge B-1 runs (i.e., K-way merge)</p>
</li>
<li>
<p>B+ tree sorting (clustered / unclustered)</p>
</li>
<li>
<p>Optimization</p>
<ul>
<li>Chunk I/O into large blocks to amortize costs.</li>
<li>Double-buffering to overlap CPU and I/O.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Aggregations</p>
<blockquote>
<p>We don’t always need the data to be ordered, so hashing is a better alternative (only need to remove duplicate, no need for sorting and cheaper computation).</p>
</blockquote>
<ul>
<li>
<p>Sorting</p>
</li>
<li>
<p>Hashing</p>
<ul>
<li>
<p>External hashing aggregation</p>
<p>Phase #1: partition</p>
<p>Divide tuples into buckets based on hash key and write them out to disk when they get full.</p>
<p>Phase #2: reHash</p>
<p>Build in-memory hash table for each partition and compute the aggregation</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Join</p>
<blockquote>
<p>Join vs Cross-Product</p>
<p>Join is the most common operation and thus must be carefully optimized.</p>
<p>Cross-Product followed by a selection is inefficient because the cross-product is large.</p>
</blockquote>
<ul>
<li>
<p>Cost Analysis Criteria (determine whether one join algorithm is better than another)</p>
<p>Metric: # of IOs to compute join</p>
</li>
<li>
<p>Join Algorithm</p>
<ul>
<li>Nested loop join</li>
<li>Sort-Merge join</li>
<li>Hash join</li>
</ul>
</li>
</ul>
<p>For table R owns M pages m tuples and table S owns N pages n tuples.</p>
<img src="image-20210425225047185.png" alt="image-20210425225047185" style="zoom:50%;" />
</li>
</ul>
</li>
<li>
<p>Query Processing Models</p>
<ul>
<li>
<p>Iterator model (Volcano / Pipeline)</p>
<p>The operator implements a loop that <strong>calls Next on its children</strong> to retrieve their tuples and then process them.</p>
</li>
<li>
<p>Materialization model (better for OLTP)</p>
<p>Each operator processes its input all at once and then emits its output all at once.</p>
</li>
<li>
<p>Vectorized / Batch model (better for OLAP)</p>
<p>Like the iterator model, but each operator emits a batch of tuples instead of a single tuple.</p>
</li>
</ul>
<p>Access method</p>
<ul>
<li>
<p>Sequential Scan</p>
<p>Optimizations</p>
<ul>
<li>
<p>Prefetching</p>
</li>
<li>
<p>Buffer Pool Bypass</p>
</li>
<li>
<p>Parallelization</p>
</li>
<li>
<p>Heap Clustering</p>
</li>
<li>
<p>Zone Maps</p>
<p>Pre-computed aggregates for the attribute values in a page.</p>
</li>
<li>
<p>Late Materialization</p>
<p>Delay stitching together tuples until the upper parts of the query plan.</p>
</li>
</ul>
</li>
<li>
<p>Index Scan</p>
<p>Picks an index to find the tuples that the query needs.</p>
</li>
<li>
<p>Multi-Index / “Bitmap” Scan</p>
<p>If there are multiple indexes that the DBMS can use for a query</p>
<ul>
<li>Compute sets of record ids using each matching index</li>
<li>Combine these sets based on the query’s predicates (union / intersect)</li>
<li>Retrieve the records and apply any remaining predicates.</li>
</ul>
</li>
</ul>
<p><strong>Halloween problem</strong> can occur on clustered tables or index scans.</p>
<blockquote>
<p>Anomaly where an update operation changes the physical location of a tuple, which causes a scan operator to visit the tuple multiple times.</p>
</blockquote>
<p>The DBMS represents a WHERE clause as an <strong>expression tree (flexible but slow)</strong> for expression evaluation.</p>
</li>
<li>
<p>Runtime Architectures</p>
<ul>
<li>
<p>Process per DBMS Worker</p>
<p>Relies on OS scheduler and shared memory.</p>
<p>A process crash doesn’t take down entire system.</p>
</li>
<li>
<p>Process pool</p>
<p>Relies on OS scheduler and shared memory.</p>
<p>Bad for CPU cache locality.</p>
</li>
<li>
<p>Thread per DBMS Worker</p>
<p>DBMS manages its own scheduling, may or may not use a dispatcher thread.</p>
<p>Thread crash (may) kill the entire system.</p>
</li>
</ul>
</li>
<li>
<p>Parallel Query Execution</p>
<ul>
<li>
<p>Execution Parallelism</p>
<ul>
<li>
<p>Inter-Query (Different queries are executed concurrently)</p>
<p>If multiple queries are updating the database at the same time, concurrency control is involved</p>
</li>
<li>
<p>Intra-Query (Execute the operations of a single query in parallel)</p>
<ul>
<li>
<p>Intra-Operator (Horizontal)</p>
<p>Decompose operators into independent fragments that perform the same function on different subsets of data.</p>
</li>
<li>
<p>Inter-Operator (Vertical) (Pipelined parallelism)</p>
<p>Operations are overlapped in order to pipeline data from one stage to the next without materialization.</p>
</li>
<li>
<p>Bushy</p>
<p>Extension of inter-operator parallelism. Exchange nodes inserted over essentially independent query plan fragments allow those fragments to execute independently of one another.</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>I/O Parallelism</p>
<ul>
<li>Multiple Disks per Database</li>
<li>One Database per Disk</li>
<li>One Relation per Disk</li>
<li>Split Relation across Multiple Disks</li>
</ul>
<p>Partition</p>
<blockquote>
<p><strong>Split single logical table</strong> into disjoint physical segments that are stored/managed separately.</p>
</blockquote>
<ul>
<li>vertical partitioning (store a table’s attributes in a separate location)</li>
<li>horizontal partitioning (divide the tuples of a table up into disjoint segments based on some partitioning key)
<ul>
<li>hash partitioning</li>
<li>range partitioning</li>
<li>predicate partitioning</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Query-planning">Query planning</h2>
<img src="image-20210425235809978.png" alt="image-20210425235809978" style="zoom:50%;" />
<ul>
<li>
<p>Heuristics / Rules (Rewrite the query to remove stupid / inefficient things)</p>
<p>Two relational algebra expressions are <strong>equivalent</strong> if they generate the same set of tuples. The DBMS can identify better query plans without a cost model. This is often called <strong>query rewriting</strong>.</p>
<blockquote>
<p>An optimizer implemented using <strong>if/then/else clauses or a pattern-matching rule engine</strong>, transforms a query’s expressions (e.g., WHERE clause predicates) into the optimal/minimal set of expressions.</p>
</blockquote>
<ul>
<li>
<p>Logical query optimization</p>
<ol>
<li>
<p>split conjunctive predicates</p>
<p>Decompose predicates into their simplest forms to make it easier for the optimizer to move them around.</p>
</li>
<li>
<p>predicate pushdown</p>
<p>Move the predicate to the lowest point in the plan after Cartesian products.</p>
</li>
<li>
<p>replace cartesian products with joins</p>
<p>Replace all Cartesian Products with inner joins using the join predicates</p>
</li>
<li>
<p>projection pushdown</p>
<p>Eliminate redundant attributes before pipeline breakers to reduce materialization cost.</p>
</li>
</ol>
<p>For nested sub-queries</p>
<ul>
<li>rewrite to de-correlate / flatten them</li>
<li>decomposed nested query and store result to temporary table</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Cost-based Search (Use a model to estimate the cost of executing a plan)</p>
<ul>
<li>
<p>cost model component</p>
<ul>
<li>Physical costs (predict CPU cycles, I/O, … and it is heavily depends on <strong>hardware</strong>)</li>
<li>Logical costs (estimate operator result sizes and independency of the operator algorithm)</li>
<li>Algorithm costs (complexity of the operator algorithm implementation)</li>
</ul>
</li>
<li>
<p>selectivity estimations</p>
<p>statistic:</p>
<p>For each relation R, the DBMS maintains the following information:</p>
<ul>
<li>NRNR : Number of tuples in R.</li>
<li>V(A,R)V(A,R): Number of distinct values for attribute A.</li>
</ul>
<p>The selection cardinality SC(A,R) is the average number of records with a value for an attribute A given NR/V(A,R)NR/V(A,R)</p>
<p>The <em>selectivity</em> (sel) of a predicate P is the fraction of tuples that qualify.</p>
<p>Formula depends on type of predicate (Equality / Range / Negation / Conjunction / Disjunction) and relies on the following assumptions</p>
<ul>
<li>
<p>Assumption #1: Uniform data</p>
<p>The distribution of values (except for the heavy hitters) is the same.</p>
</li>
<li>
<p>Assumption #2: Independent predicates</p>
<p>The predicates on attributes are independent.</p>
</li>
<li>
<p>Assumption #3: Inclusive principle</p>
<p>The domain of join keys overlap such that each key in the inner relation will also exist in the outer table.</p>
</li>
</ul>
</li>
<li>
<p>non-uniform approximation</p>
<ul>
<li>equi-width histogram (all buckets have the same width)</li>
<li>equi-depth histogram (vary the width of buckets so that the total number of occurrences for each bucket is roughly the same)</li>
<li>sketches (probabilistic data structures that generate approximate statistics about a data set)</li>
<li>sampling</li>
</ul>
</li>
<li>
<p>query optimization</p>
<ul>
<li>
<p>Single relation query planning</p>
<ol>
<li>Pick the best access method.</li>
<li>Predicate evaluation ordering.</li>
</ol>
</li>
<li>
<p>Multiple relation query planning</p>
<ol>
<li>Enumerate relation orderings (need to restrict search space)</li>
<li>Enumerate the plans for each operator</li>
<li>Enumerate access method choices for each table</li>
</ol>
<p>Use dynamic programming to reduce the number of cost estimations.</p>
</li>
<li>
<p>Nested sub-queries</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="image-20210427234935009.png" alt="image-20210427234935009" style="zoom:50%;" />
<h2 id="Concurrency-Control">Concurrency Control</h2>
<h3 id="Transaction">Transaction</h3>
<blockquote>
<p>Correct criteria</p>
<p>Atomicity Consistency Isolation Durability</p>
</blockquote>
<ul>
<li>
<p>Atomicity</p>
<ul>
<li>Logging</li>
<li>Shadow Paging</li>
</ul>
</li>
<li>
<p>Consistency</p>
<ul>
<li>
<p>database consistency (accurately models the real world and <strong>follows integrity constraints</strong>)</p>
</li>
<li>
<p>transaction consistency</p>
<p>If the database is consistent before the transaction starts (running alone), it will also be consistent after. It is the <strong>application’s responsibility</strong>.</p>
</li>
</ul>
</li>
<li>
<p>Isolation</p>
<p>A concurrency control protocol is how the DBMS decides the proper interleaving of operations from multiple transactions.</p>
<ul>
<li>pessimistic</li>
<li>optimistic</li>
</ul>
<hr>
<p>Interleaved execution anomalies</p>
<ul>
<li>Read-Write Conflict -&gt; unrepeatable read</li>
<li>Write-Read Conflict -&gt; dirty read</li>
<li>Write-Write Conflict -&gt; overwrite uncommitted data</li>
</ul>
<hr>
<p>Schedule correctness judgement</p>
<blockquote>
<p>If a schedule is equivalent to some serial execution, we judge it is correct.</p>
</blockquote>
<ul>
<li>
<p>Serial schedule</p>
<p>A schedule that does not interleave the actions of different transactions.</p>
</li>
<li>
<p>Equivalent schedule</p>
<p>For any database state, the effect of executing the first schedule is identical to the effect of executing the second schedule.</p>
</li>
<li>
<p>Serializable schedule</p>
<p>A schedule that is equivalent to some serial execution of the transactions.</p>
</li>
</ul>
<p>Different levels of serializability</p>
<ul>
<li>
<p>Conflict Serializability (most DBMSs try to support it)</p>
<p>Verify using either the “swapping” method or dependency graphs.</p>
</li>
<li>
<p>View Serializability (no DBMS can do this)</p>
<p>No efficient way to verify.</p>
</li>
</ul>
<p>Serial ⫋⫋ Conflict Serializable ⫋⫋ View Serializable ⫋⫋ All Schedules</p>
</li>
<li>
<p>Durability</p>
<ul>
<li>logging</li>
<li>shadow paging</li>
</ul>
</li>
</ul>
<h3 id="Pessimistic">Pessimistic</h3>
<ul>
<li>
<p>Basic Lock Type:</p>
<ul>
<li>S-LOCK</li>
<li>X-LOCK</li>
</ul>
</li>
<li>
<p>Two-phase locking</p>
<p>Phase #1. Growing</p>
<p>Phase #2. Shrinking</p>
<p>Problem: it is subject to cascading aborts.</p>
</li>
<li>
<p>Strong strict 2PL</p>
<p>Release all locks at end of txn.</p>
<p>Allows only conflict serializable schedules.</p>
</li>
<li>
<p>2PL deadlocks</p>
<ul>
<li>
<p>deadlock detection (waits-for graph)</p>
<ul>
<li>deadlock handling:
<ul>
<li>victim selection</li>
<li>rollback length: completely or minimally</li>
</ul>
</li>
</ul>
</li>
<li>
<p>deadlock prevention</p>
<p>Assign priorities based on timestamps: Older Timestamp = Higher Priority</p>
<ul>
<li>wait-die</li>
</ul>
<p>If requesting txn has higher priority than holding txn, then requesting txn waits for holding txn.</p>
<ul>
<li>wound-wait</li>
</ul>
<p>If requesting txn has higher priority than holding txn, then holding txn aborts and releases lock.</p>
</li>
</ul>
</li>
<li>
<p>Lock granularities</p>
<p>Trade-off between parallelism versus overhead.</p>
<p>Fewer locks, larger granularity vs. more locks, smaller granularity</p>
<ul>
<li>
<p>intention locks</p>
<ul>
<li>
<p>Intention-Shared (IS)</p>
</li>
<li>
<p>Intention-Exclusive (IX)</p>
</li>
<li>
<p>Shared+Intention-Exclusive (SIX)</p>
<img src="image-20210430020350478.png" alt="image-20210430020350478" style="zoom:50%;" />
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Optimistic">Optimistic</h3>
<p>Timestamp Ordering (T/O)</p>
<ul>
<li>
<p>timestamp allocation</p>
<ul>
<li>system clock</li>
<li>logical counter</li>
<li>hybrid</li>
</ul>
</li>
<li>
<p>Basic Timestamp Ordering (T/O) Protocol</p>
<p>Thomas Write Rule (reduce abort) :</p>
<p>If TS(Ti) &lt; W-TS(X), ignore the write to allow the txn to continue executing without aborting.</p>
</li>
<li>
<p>Optimistic Concurrency Control</p>
<ol>
<li>Read Phase</li>
<li>Validation Phase
<ul>
<li>Backward validation</li>
<li>Forward validation</li>
</ul>
</li>
<li>Write Phase</li>
</ol>
</li>
<li>
<p>Phantom problem</p>
<ul>
<li>Re-Execute Scans</li>
<li>Predicate Locking</li>
<li>Index Locking</li>
</ul>
</li>
</ul>
<p><img src="image-20210428031329246.png" alt="image-20210428031329246" style="zoom:50%;" /><img src="image-20210429003431694.png" alt="image-20210429003431694" style="zoom:50%;" /></p>
<h3 id="MVCC">MVCC</h3>
<blockquote>
<p>Snapshot isolation</p>
<p>Write skew anomaly</p>
</blockquote>
<ul>
<li>
<p>Concurrency control protocol (the default is Timestamp ordering)</p>
</li>
<li>
<p>Version storage</p>
<ul>
<li>
<p>Append only storage</p>
<p>chain ordering (oldest to newest / newest to oldest)</p>
</li>
<li>
<p>Time-travel storage</p>
</li>
<li>
<p>Delta storage</p>
</li>
</ul>
</li>
<li>
<p>Garbage collection</p>
<ul>
<li>
<p>Look for expired versions</p>
<ul>
<li>
<p>Version tracking</p>
<ul>
<li>
<p>Tuple level</p>
<p>background vacuuming vs. cooperative cleaning</p>
</li>
<li>
<p>Transaction level</p>
</li>
<li>
<p>Epochs</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Decide when it is safe to reclaim memory</p>
<ul>
<li>Frequency
<ul>
<li>Periodically</li>
<li>Continuously</li>
</ul>
</li>
<li>Granularity
<ul>
<li>Single version</li>
<li>Group version</li>
<li>Tables</li>
</ul>
</li>
<li>Comparison unit (determine whether version(s) are reclaimable)
<ul>
<li>Timestamp</li>
<li>Interval</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Index management</p>
<ul>
<li>Logical pointers</li>
<li>Physical pointers</li>
</ul>
</li>
</ul>
<p>Modern MVCC Implementation</p>
<ul>
<li>Hekaton (SQL Server)</li>
<li>TUM HyPer</li>
<li>SAP HANA</li>
<li>CMU Cicada</li>
</ul>
<h2 id="Recovery">Recovery</h2>
<blockquote>
<p>Ensure database consistency, transaction atomicity, and durability despite failures</p>
</blockquote>
<ol>
<li>
<p>Actions during normal txn processing to ensure that the DBMS can recover from a failure.</p>
<ul>
<li>
<p>Failure classification</p>
<ul>
<li>
<p>Transaction failures</p>
<ul>
<li>
<p>logical errors</p>
<p>Transaction cannot complete due to some internal error condition (e.g., integrity constraint violation).</p>
</li>
<li>
<p>internal state errors</p>
<p>DBMS must terminate an active transaction due to an error condition (e.g., deadlock).</p>
</li>
</ul>
</li>
<li>
<p>System failures</p>
<ul>
<li>
<p>software failure</p>
<p>Problem with the OS or DBMS implementation (e.g., uncaught divide-by-zero exception).</p>
</li>
<li>
<p>hardware failure</p>
<p>The computer hosting the DBMS crashes (e.g., power plug gets pulled).</p>
</li>
</ul>
</li>
<li>
<p>Storage media failures</p>
<ul>
<li>
<p>non-repairable hardware failure</p>
<p>A head crash or similar disk failure destroys all or part of non-volatile storage.</p>
<p>Destruction is assumed to be detectable (e.g., disk controller use checksums to detect failures)</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Undo &amp; Redo</p>
<p>Undo: The process of removing the effects of an incomplete or aborted txn.</p>
<p>Redo: The process of re-instating the effects of a committed txn for durability.</p>
<ul>
<li>
<p>Steal Policy</p>
<blockquote>
<p>Whether the DBMS allows an uncommitted txn to overwrite the most recent committed value of an object in non-volatile storage</p>
</blockquote>
</li>
<li>
<p>Force Policy</p>
<blockquote>
<p>Whether the DBMS requires that all updates made by a txn are reflected on non-volatile storage before the txn can commit.</p>
</blockquote>
</li>
</ul>
<p>no-steal + force -&gt; <strong>shadow paging</strong></p>
<ul>
<li>
<p>disadvantages:</p>
<p>copying the entire page table is expensive</p>
<p>commit overhead is high</p>
</li>
</ul>
<p>steal + no-force -&gt; <strong>write-ahead log</strong></p>
</li>
<li>
<p>Logging schema</p>
<ul>
<li>physical logging</li>
<li>logical logging</li>
<li>physiological logging</li>
</ul>
</li>
<li>
<p>Checkpoints</p>
</li>
</ul>
</li>
<li>
<p>Actions after a failure to recover the database to a state that ensures atomicity, consistency, and durability.</p>
<blockquote>
<p>Mains ideas of <strong>ARIES</strong></p>
<ol>
<li>WAL with steal / no-force</li>
<li>fuzzy checkpoints (snapshot of dirty page ids)</li>
<li>Redo everything since the earliest dirty page</li>
<li>Undo txns that never commit</li>
<li>Write CLRs when undoing, to survive failures during restarts</li>
</ol>
</blockquote>
<ol>
<li>
<p>Write-Ahead Logging</p>
<ul>
<li>
<p>Log Sequence number</p>
<img src="image-20210429001556373.png" alt="image-20210429001556373" style="zoom:50%;" />
</li>
</ul>
</li>
<li>
<p>Fuzzy Checkpointing</p>
<ul>
<li>
<p>Non-fuzzy checkpoints</p>
<p>The DBMS halts everything when it takes a checkpoint to ensure a consistent snapshot.</p>
</li>
<li>
<p>Slightly better checkpoints</p>
<ul>
<li>Active transaction table (ATT)</li>
<li>Dirty Page Table (DPT)</li>
</ul>
</li>
<li>
<p>Fuzzy checkpoints</p>
<p>Allows active txns to continue the run while the system flushes dirty pages to disk</p>
<p>Checkpoint boundaries</p>
<ul>
<li>begin (indicates start of checkpoint)</li>
<li>end (contains ATT+DPT)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Recovery phase</p>
<p>Phase #1: Analysis</p>
<p>Phase #2: Redo</p>
<p>Phase #3: Undo</p>
</li>
</ol>
</li>
</ol>
<h2 id="Others-Compression">Others: Compression</h2>
<ul>
<li>
<p>granularity</p>
<ul>
<li>block level</li>
<li>tuple level</li>
<li>attribute level</li>
<li>column level</li>
</ul>
</li>
<li>
<p>columnar compression</p>
<ul>
<li>
<p>null supression</p>
<p>Consecutive zeros or blanks in the data are replaced with a description of how many there were and where they existed.</p>
</li>
<li>
<p>run-length encoding</p>
<p>Compress runs of the same value in a single column into triplets:</p>
<ol>
<li>the value of the attribute</li>
<li>the start position in the column segment</li>
<li>the # of elements in the run</li>
</ol>
<p>Requires the columns to be sorted intelligently to maximize compression opportunities.</p>
</li>
<li>
<p>bitmap encoding</p>
<p>Store a separate bitmap for each unique value for an attribute where an offset in the vector corresponds to a tuple.</p>
</li>
<li>
<p>delta encoding</p>
<p>Recording the difference between values that follow each other in the same column.</p>
</li>
<li>
<p>incremental encoding</p>
<p>Type of delta encoding that avoids duplicating common prefixes/suffixes between consecutive tuples. This works best with sorted data.</p>
</li>
<li>
<p>mostly encoding</p>
<p>When values for an attribute are “mostly” less than the largest size, store them as smaller data type</p>
</li>
<li>
<p>dictionary encoding</p>
<p>Replace frequent patterns with smaller codes.</p>
<ul>
<li>dictionary construction
<ul>
<li>all at once</li>
<li>incremental</li>
</ul>
</li>
<li>dictionary scope
<ul>
<li>block level</li>
<li>table level</li>
<li>multi-table</li>
</ul>
</li>
<li>dictionary data structures
<ul>
<li>array</li>
<li>hash table</li>
<li>b+ tree</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1>Reference</h1>
<p><a href="https://15445.courses.cs.cmu.edu/fall2020/">CMU 15-445/645 FALL 2020 DATABASE SYSTEMS</a></p>
<p><a href="https://15721.courses.cs.cmu.edu/spring2020/">CMU 15-721 SPRING 2020 Advanced Database Systems</a></p>
]]></content>
      <categories>
        <category>Concepts</category>
      </categories>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title>handy源码分析</title>
    <url>/2020/04/18/handy/</url>
    <content><![CDATA[<h1>Handy网络库分析</h1>
<p>最近想学一下Linux的网络编程，于是想先阅读一些网络库，所以这篇博客可能会有一些比较基本的知识点。</p>
<p>直接开始，我们先来看看利用Doxygen生成的Include依赖关系图</p>
<p><img src="includes.png" alt="include-relationship"></p>
<p>我们就按从下往上来分析一下各个头文件所实现的功能</p>
<h2 id="slice-h">slice.h</h2>
<p>定义字符串切片类（包含front() / back() / begin() / end() 等类似容器的操作，但实际的实现是char*）以及切片的一些操作（返回第一个词，返回第一行，返回前n个字符等等）</p>
<h2 id="port-posix-h">port_posix.h</h2>
<p>用于解决主机字节序到网络字节序的转换，根据 OS_LINUX / OS_MACOSX 实现不同的getHostByName</p>
<h2 id="util-h">util.h</h2>
<blockquote>
<p>这里noncopyable是通过将<strong>拷贝构造函数</strong>和<strong>拷贝赋值运算符</strong>声明为禁止使用(=delete)实现</p>
<p>这样实现的好处：</p>
<ul>
<li>在c++11之前实现该功能需要将这两个函数设为private，但依然无法避免成员函数和友元函数的使用</li>
<li>通过更少/更明确的代码实现，且可复用(从noncopyable类派生)</li>
</ul>
<p>该类的作用：</p>
<ul>
<li>防止对象被拷贝。尤其是在涉及到文件操作/网络连接的类中，资源的释放/关闭可能会出现问题</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//boost::noncopyable</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">noncopyable</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="built_in">noncopyable</span>() = <span class="keyword">default</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="built_in">noncopyable</span>(<span class="type">const</span> noncopyable&amp;) = <span class="keyword">delete</span>;</span><br><span class="line">  noncopyable&amp; <span class="keyword">operator</span>=(<span class="type">const</span> noncopyable&amp;) = <span class="keyword">delete</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>具体解释可看<a href="https://docs.microsoft.com/en-us/cpp/cpp/explicitly-defaulted-and-deleted-functions?view=vs-2019">Explicitly Defaulted and Deleted Functions</a></p>
</blockquote>
<p>util类则包含</p>
<ul>
<li>
<p>格式化输出string</p>
<blockquote>
<p>进行两次尝试：</p>
<p>第1次尝试：</p>
<p>申请char[500]的栈内存，va_start获取可变参数列表，通过vsnprintf()得到格式化后的字符串，判断返回大小是否超出限制，未超出结束尝试并返回，超出则进行第二次尝试</p>
<p>第2次尝试：</p>
<p>申请char[30000]的堆内存，并且由<strong>unique_ptr</strong>管理(销毁)，通过vsnprintf()得到格式化后的字符串，判断返回大小是否超出限制，未超出结束尝试并返回，超出则截断，将字符串最后一字节设为结束字符‘\0’</p>
</blockquote>
</li>
<li>
<p>基础的时间函数（std::chrono库）</p>
</li>
<li>
<p>atoi() ---- 将字符串转换为整数，函数内部通过strtol()实现</p>
</li>
<li>
<p>更改(添加)文件描述符flag（fcntl）</p>
</li>
</ul>
<p>ExitCaller类继承于noncopyable，只有一个私有成员<code>std::function&lt;void()&gt; functor_</code>，即一个无参数返回int的可调用对象，并且在析构时进行调用执行。当实例为局部变量时，作用与golang的defer类似</p>
<h2 id="net-h">net.h</h2>
<p><code>net</code>类包含一些设置函数</p>
<ul>
<li>
<p>setNonBlock()设置文件的 阻塞/非阻塞IO</p>
</li>
<li>
<p>setReuseAddr() / setReusePort() / setNoDelay() 设置套接字的属性</p>
<blockquote>
<p><code>SO_REUSEADDR</code>允许 绑定一个正处于TIME_WAIT中的本地地址 / 通配符IP(INADDR_ANY)地址冲突</p>
<p><code>SO_REUSEPORT</code>允许多个套接字绑定到同一地址端口组合(所有套接字必须都设置了<code>SO_REUSEPORT</code>)</p>
<p><code>TCP_NODELAY</code> 关闭Nagle缓存算法，允许小包的发送(适合延时敏感且数据量较小的情况)</p>
<p><a href="http://man7.org/linux/man-pages/man7/socket.7.html">man socket</a></p>
<p><a href="https://stackoverflow.com/questions/14388706/how-do-so-reuseaddr-and-so-reuseport-differ">How do SO_REUSEADDR and SO_REUSEPORT differ?</a></p>
</blockquote>
</li>
</ul>
<p><code>Ip4Addr</code>类则是对sockaddr_in进行了一些简单的封装，提供一些常用的函数(format/getter/validate)</p>
<p><code>Buffer</code>类是连续的动态伸缩的内存空间，用于缓存</p>
<p>内存空间大小由私有成员变量</p>
<p><code>char *buf_</code>内存地址起始地址 / <code>size_t b_</code>正在使用内存空间（缓存数据但未使用）的起始地址相对buf的偏移量 / <code>size_t e_</code>正在使用内存空间的结束地址 + 1数据单元地址相对buf的偏移量 / <code>size_t cap_</code>已分配空间大小 / <code>size_t exp_</code>建议/期望分配大小</p>
<ul>
<li>
<p>添加数据：先调用<code>allocRoom(len)</code>分配足够大小空间，然后<code>memcpy()</code>拷贝数据</p>
</li>
<li>
<p>分配空间：调用<code>makeRoom(len)</code>获得所分配空间的首地址</p>
<p>在分配过程中，如果内存空间足够(<code>e_ + len &lt;= cap_</code>)，无需操作</p>
<p>如果内存尾部空间不足</p>
<ol>
<li>判断当前所需要使用的内存是否小于已分配的一半(<code>size() + len &lt; cap_/2</code>)，是则将有效数据移动到起始位置<code>buf_</code></li>
<li>调用<code>expand(len)</code>重新分配一块内存，内存大小为<code>max(exp_, max(2 * cap_, size() + len))</code></li>
</ol>
</li>
<li>
<p>获取数据：<code>consume(size_t len)</code>直接<code>b_ += len</code>，所以使用时需保证<code>len</code>的大小正确</p>
</li>
</ul>
<h2 id="codec-h">codec.h</h2>
<blockquote>
<p>TCP的粘包/拆包问题：TCP是字节流协议，消息之间没有边界</p>
<p>一般的处理：</p>
<ul>
<li>发送定长包。如果每个消息的大小都是一样的，那么在接收对等方只要累计接收数据，直到数据等于一个定长的数值就将它作为一个消息。</li>
<li>包尾加上\r\n标记。FTP协议正是这么做的。但问题在于如果数据正文中也含有\r\n，则会误判为消息的边界。</li>
<li>包头加上包体长度。包头是定长的4个字节，说明了包体的长度。接收对等方先接收包体长度，依据包体长度来接收包体。</li>
<li>使用更加复杂的应用层协议</li>
</ul>
</blockquote>
<p><code>CodecBase</code>类 就是用来处理这些问题，其派生类<code>LineCodec</code>采用第二种方案，<code>LengthCodec</code>采用第三种方案</p>
<p><code>LengthCodec</code>类的具体编码：“mBdt”（四个字节）+ 消息长度（四个字节）+消息</p>
<h2 id="threads-h">threads.h</h2>
<p>SafeQueue类</p>
<p>要做到线程安全，我们需要利用锁。一般在封装线程安全的类时，我们会直接声明一个mutex成员变量，但是该类是通过私有继承mutex类实现，所以上锁的时候直接传入(*this)：<code>lock_guard&lt;mutex&gt; lk(*this)</code></p>
<p><code>push()</code>任务时直接上锁，然后将任务添加到链表中，调用<code>notify_one()</code>唤醒一个等待条件变量<code>ready_</code>的线程</p>
<p><code>wait_ready()</code>使用条件变量阻塞线程，直到 超时 / 队列中不为空(有任务) / 退出</p>
<p><code>pop_wait()</code>调用<code>wait_ready()</code>，返回空任务(退出/超时)或者队列头任务(队列不为空)</p>
<p>ThreadPool类</p>
<p>构造一个最大等待任务数量为<code>taskCapacity</code>，线程数量为<code>thread</code>的线程池，当线程启动时，遍历线程集合，对于每一个线程，创建一个新的线程执行<code>pop_wait()</code>获取任务并执行，然后交换，即将新线程替换旧线程</p>
<h2 id="logging-h">logging.h</h2>
<p>定义了8个日志等级</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum</span> <span class="title class_">LogLevel</span> &#123; LFATAL = <span class="number">0</span>, LERROR, LUERR, LWARN, LINFO, LDEBUG, LTRACE, LALL &#125;;</span><br></pre></td></tr></table></figure>
<p>Logger类是通过<code>getLogger()</code>获取一个静态单例（c++11保证局部静态变量的初始化是线程安全的），功能</p>
<ol>
<li>
<p>可以绝对<code>setLogLevel()</code> / 相对<code>adjustLogLevel()</code>设置 / 获取<code>getLogLevel()</code>当前的日志输出等级</p>
</li>
<li>
<p>自定义设置日志文件，打开方式</p>
<ul>
<li><code>O_APPEND</code>将写入追加到文件的尾端</li>
<li><code>O_CREAT</code>不存在则自动创建，权限为<code>#define DEFFILEMODE (S_IRUSR|S_IWUSR|S_IRGRP|S_IWGRP|S_IROTH|S_IWOTH)</code> 即 User/Group/Others可读可写</li>
<li><code>O_WRONLY</code>只写</li>
<li><code>O_CLOEXEC</code>设置文件描述符<code>close-on-exec</code>，即当fork()的子进程成功调用exec()类函数时，会自动关闭该文件描述符，避免泄露给子进程</li>
</ul>
<blockquote>
<p>修改成员函数的时候使用的是<code>dup2()</code>，避免<code>close()</code>+<code>dup()</code>两个函数会受到竞争条件的影响：比如在两个函数之间，成员函数被赋值为另一个fd</p>
</blockquote>
</li>
<li>
<p>设置日志周期<code>rotateInterval_</code></p>
<ul>
<li>如果当前时间与当前日志文件创建时间在同一周期内，无需新建日志文件</li>
<li>否则，保存并重命名为 当前年月日时分</li>
</ul>
</li>
<li>
<p>写入日志<code>logv(int level, const char *file, int line, const char *func, const char *fmt...)</code></p>
<ul>
<li>如果写入日志等级高于当前日志等级，退出</li>
<li>检查是否开启新的日志周期（重命名）</li>
<li>写入详细的时间 + tid + 发生的文件/位置 + 事件内容 + ‘\n’</li>
</ul>
</li>
</ol>
<h2 id="status-h">status.h</h2>
<p>表示文件操作后的状态</p>
<p>表示格式：</p>
<p>state_[0…3] == length of state_</p>
<p>state_[4…7] == code</p>
<p>state_[8…] == message</p>
<h2 id="file-h">file.h</h2>
<p>file类中包含了一些文件io的静态成员函数，返回<code>Status</code>类</p>
<ul>
<li>
<p><code>getContent(const string &amp;filename, string &amp;cont)</code>：将文件内容读(append)到字符串中</p>
</li>
<li>
<p><code>writeContent(const string &amp;filename, const string &amp;cont)</code>：截断并从头往文件写入字符串中内容</p>
</li>
<li>
<p><code>createDir(const string &amp;name)</code>/ <code>deleteDir(const string &amp;name)</code>：创建/删除文件目录</p>
</li>
<li>
<p><code>getChildren(const string &amp;dir, vector&lt;string&gt; *result)</code>：通过<code>readdir()</code>获取遍历目录文件信息，然后它们的文件名 <code>push_back(d_name)</code>到<code>*result</code></p>
</li>
<li>
<p><code>getFileSize((const string &amp;fname, uint64_t *size))</code>：通过调用<code>stat(const char *path, struct stat *buf)</code> 获取文件大小，并将<code>*size</code>赋值为文件大小<code>buf.st_size</code></p>
</li>
<li>
<p><code>deleteFile(const string &amp;fname)</code>：通过调用<code>unlink(const char *pathname)</code>删除文件</p>
<blockquote>
<p><strong>unlink</strong>() 从文件系统删除一个名字.</p>
<ul>
<li>如果这个名字是指向文件的最后一个链接并且该文件当前没有被某进程打开，则删除该文件</li>
<li>如果这个名字是指向文件的最后一个链接并且该文件正在被某进程打开，则等待指向该文件的最后一个文件描述符被关闭时删除该文件</li>
<li>如果这个名字指向一个软链接，则删除该软连接</li>
<li>如果名称指向一个socket、fifo或设备，则删除其名称，但打开该对象的进程可以继续使用它。</li>
</ul>
</blockquote>
</li>
<li>
<p>调用<code>writeContent(tmpName,cont)</code> 往 <code>tmpName</code>文件写入内容成功后，调用<code>renameFile(tmpName,name)</code>重命名</p>
</li>
</ul>
<h2 id="handy-imp-h">handy_imp.h</h2>
<p><code>AutoContext</code>类通过模板类实现对传入类的内存管理，自动（第一次访问时）new和（析构时）delete</p>
<h2 id="poller-h">poller.h</h2>
<blockquote>
<p>结合event_base一起看</p>
</blockquote>
<p>基于<code>PollerBase</code>抽象类，根据Linux和MacOS不同平台派生类，实现了对epoll的封装</p>
<h3 id="epoll">epoll</h3>
<blockquote>
<p><code>epoll</code>把用户关心的文件描述符上的事件放在内核里的一个事件表中，从而无需像 <code>select</code> 和 <code>poll</code> 那样每次调用都要重复传入文件描述符集或事件集。但 <code>epoll</code> 需要使用一个额外的文件描述符来<strong>唯一标识</strong>内核中的这个事件表</p>
</blockquote>
<p>这里我们先来看看epoll的几个函数</p>
<ul>
<li>
<p><code>int epoll_create(int size)</code>：用来创建上述的文件描述符。<code>size</code>参数现在不起作用，只是给内核一个提示，告诉它事件表需要多大</p>
</li>
<li>
<p><code>int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)</code>：</p>
<p><code>fd</code> 是要操作的文件描述符，<code>op</code> 参数则指定操作类型，有如下3种：</p>
<ul>
<li><code>EPOLL_CTL_ADD</code>：往事件表种注册 <code>fd</code> 上的事件</li>
<li><code>EPOLL_CTL_MOD</code>：修改 <code>fd</code> 上的注册事件</li>
<li><code>EPOLL_CTL_DEL</code>：删除 <code>fd</code> 上的注册事件</li>
</ul>
<p><code>event</code> 参数指定事件，<code>epoll_event</code> 结构体定义了：<code>events</code> 成员描述事件类型，<code>data</code> 成员存储用户数据</p>
</li>
<li>
<p><code>int epoll_wait(int epfd, struct epoll_event* events, int maxevents, int timeout)</code>：</p>
<p><code>epoll_wait()</code>在一段超时时间内等待一组文件描述符上的事件，<code>timeout</code> 指定超时值，单位是ms，<code>maxevents</code> 指定最多监听多少个事件</p>
<p><code>epoll_wait()</code> 如果检测到事件，就将所有就绪的事件从内核事件表中复制它的第二个参数<code>events</code>指向的数组中</p>
</li>
</ul>
<h4 id="Level-Trigger">Level Trigger</h4>
<blockquote>
<p>当<code>epoll_wait()</code> 检测到有事件发生并将其通知应用程序后，应用程序可以不立即处理该事件。当应用程序下一次调用 <code>epoll_wait()</code> 时，此事件仍然会被通知，直到该事件被处理</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">lt</span><span class="params">(epoll_event* events, <span class="type">int</span> number, <span class="type">int</span> epollfd, <span class="type">int</span> listenfd)</span></span>&#123;</span><br><span class="line">    <span class="type">char</span> buf[BUFFER_SIZE];</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; number; i++)&#123;</span><br><span class="line">        <span class="type">int</span> sockfd = events[i].data.fd;</span><br><span class="line">        <span class="keyword">if</span>(sockfd == listenfd) &#123;	<span class="comment">/*Regist*/</span>	&#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (events[i].events &amp; EPOLLIN)&#123;</span><br><span class="line">            <span class="type">int</span> ret = <span class="built_in">recv</span>( sockfd, buf, BUFFER_SIZE - <span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line">            <span class="keyword">if</span>(ret &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">                <span class="built_in">close</span>(sockfd);</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//data processing</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Edge-Trigger">Edge Trigger</h4>
<blockquote>
<p>当<code>epoll_wait()</code> 检测到有事件发生并将其通知应用程序后，应用程序必须立即处理该事件，因为后续的<code>epoll_wait()</code> 不再向应用程序通知该事件</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">et</span><span class="params">(epoll_event* events, <span class="type">int</span> number, <span class="type">int</span> epollfd, <span class="type">int</span> listenfd)</span></span>&#123;</span><br><span class="line">    <span class="type">char</span> buf[BUFFER_SIZE];</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; number; i++)&#123;</span><br><span class="line">        <span class="type">int</span> sockfd = events[i].data.fd;</span><br><span class="line">        <span class="keyword">if</span>(sockfd == listenfd)&#123;		<span class="comment">/*Regist*/</span>	&#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(events[i].events &amp; EPOLLIN)&#123;</span><br><span class="line">            <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">                <span class="type">int</span> ret = <span class="built_in">recv</span>( sockfd, buf, BUFFER_SIZE - <span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line">                <span class="keyword">if</span>(ret &lt; <span class="number">0</span>)&#123;</span><br><span class="line">                    <span class="keyword">if</span>( (errno == EAGAIN ) || (errno == EWOULDBLOCK))<span class="keyword">break</span>;</span><br><span class="line">                    <span class="built_in">close</span>(sockfd);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(ret == <span class="number">0</span>)<span class="built_in">close</span>(sockfd);</span><br><span class="line">                <span class="keyword">else</span> &#123;	<span class="comment">/*data processing*/</span>	&#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="event-base-h">event_base.h</h2>
<blockquote>
<p>事件的到来是随机的、异步的。我们无法预知程序何时收到一个客户连接请求 / 暂停信号。所以程序需要循环等待并处理事件，这就是<strong>事件循环</strong>。在事件循环中，等待事件一般使用I/O复用技术实现。</p>
<p>将系统支持的各种I/O复用系统调用封装成统一的接口，称为<strong>事件多路分发器</strong>，一般包含三个函数：等待事件(select/poll/epoll_wait)，添加事件，删除事件。</p>
<p><strong>事件处理器</strong>执行事件对应的业务逻辑，通常包含一个或多个回调函数，这些回调函数在事件循环中被执行。</p>
</blockquote>
<p><code>TimerRepeatable</code> 类是一个带有编号，回调函数的定时器，通过其中的 <code>at</code> 过期时间，<code>interval</code>定时周期控制</p>
<p><strong>事件循环</strong>：<code>EventBase::loop()</code> -&gt; <code>EventsImp::loop()</code> -&gt; <code>poller::loop_once()</code></p>
<p><strong>事件分发器</strong>：<code>EventBase::safeCall()</code> -&gt; <code>EventsImp::safeCall()</code> -&gt;</p>
<p><code>EventsBase</code>类是对 <code>EventsImp</code> 类再进一层封装，简化操作，方便多线程事件派发器<code>MultiBase</code>的实现</p>
<p><code>EventsImp</code> 类成员</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">EventBase *base_;		<span class="comment">//所属的事件分发器</span></span><br><span class="line">PollerBase *poller_;	</span><br><span class="line">std::atomic&lt;<span class="type">bool</span>&gt; exit_;	<span class="comment">//设置退出</span></span><br><span class="line"><span class="type">int</span> wakeupFds_[<span class="number">2</span>];			<span class="comment">//用于唤醒IO线程</span></span><br><span class="line"><span class="type">int</span> nextTimeout_;			</span><br><span class="line">SafeQueue&lt;Task&gt; tasks_;</span><br><span class="line"></span><br><span class="line">std::map&lt;TimerId, TimerRepeatable&gt; timerReps_;	<span class="comment">//循环定时任务</span></span><br><span class="line">std::map&lt;TimerId, Task&gt; timers_;	<span class="comment">//定时任务</span></span><br><span class="line">std::atomic&lt;<span class="type">int64_t</span>&gt; timerSeq_;		<span class="comment">//给每个定时器编号</span></span><br><span class="line"><span class="comment">// 记录每个idle时间（单位秒）下所有的连接。链表中的所有连接，最新的插入到链表末尾。连接若有活动，会把连接从链表中移到链表尾部，做法参考memcache</span></span><br><span class="line">std::map&lt;<span class="type">int</span>, std::list&lt;IdleNode&gt;&gt; idleConns_;	<span class="comment">//空闲连接</span></span><br><span class="line">std::set&lt;TcpConnPtr&gt; reconnectConns_;		<span class="comment">//重连连接</span></span><br><span class="line"><span class="type">bool</span> idleEnabled;			<span class="comment">//是否正在idle watcher模式</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//调用poller::loop_once()，然后调用handleTimeouts()处理超时任务</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">loop_once</span><span class="params">(<span class="type">int</span> waitMs)</span></span>;</span><br><span class="line"><span class="comment">//循环调用loop_once(10000)直至设置退出，然后清空所有定时任务，清空连接，loop_once(0)</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">loop</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">//创建管道，然后向poller注册它的可读事件，用于唤醒IO线程</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//如果连接空闲时间超过idle，重置该连接的空闲时间，将其从链表头部放到尾部，执行空闲超时回调</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">callIdles</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">//启用idle watcher模式，将注册事件添加到对应idle秒的链表末尾</span></span><br><span class="line"><span class="function">IdleId <span class="title">registerIdle</span><span class="params">(<span class="type">int</span> idle, <span class="type">const</span> TcpConnPtr &amp;con, <span class="type">const</span> TcpCallBack &amp;cb)</span></span>;</span><br><span class="line"><span class="comment">//取消id对应的空闲连接监控事件</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">unregisterIdle</span><span class="params">(<span class="type">const</span> IdleId &amp;id)</span></span>;</span><br><span class="line"><span class="comment">//将id从所在的监控链表放到末尾</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">updateIdle</span><span class="params">(<span class="type">const</span> IdleId &amp;id)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//将超时的任务移除并执行，然后调用refreshNearest()</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">handleTimeouts</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">//检查距离下一次超时的时间</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">refreshNearest</span><span class="params">(<span class="type">const</span> TimerId *tid = <span class="literal">NULL</span>)</span></span>;</span><br><span class="line"><span class="comment">//对于需要重复执行的超时任务，执行之前原子更新其定时器，回调函数为重新调用repeatableTimeout()</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">repeatableTimeout</span><span class="params">(TimerRepeatable *tr)</span></span>;</span><br><span class="line"><span class="comment">//添加定时任务</span></span><br><span class="line"><span class="comment">//对于循环定时任务TimerId.first = -milli，放进 timers_ 和 timerReps_ 里</span></span><br><span class="line"><span class="comment">//对于一次定时任务TimerId.first = milli，放进 timers_ 里</span></span><br><span class="line"><span class="function">TimerId <span class="title">runAt</span><span class="params">(<span class="type">int64_t</span> milli, Task &amp;&amp;task, <span class="type">int64_t</span> interval)</span></span>;</span><br><span class="line"><span class="comment">//取消定时任务</span></span><br><span class="line"><span class="comment">//与runAt()规则一样，将其从 timers_ / timerReps_ 里删除</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">cancel</span><span class="params">(TimerId timerid)</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>EventBase</code> 类只有一个事件分发器，只在主线程上运行，所有的事件处理也是在主线程上</p>
<p><code>EventBase</code> 类只有一个事件分发器，只在主线程上运行，所有的事件处理也是在主线程上</p>
<p><code>MultiBase</code>类拥有多个事件分发器<code>EventBase</code> ，在执行<code>loop()</code>的时候，为第0 ~ n-1个事件分发器创建一个线程执行<code>EventBase::loop()</code>，第n个事件分发器（主Reactor，需处理accept）则在主线程上执行，所有的事件处理在各自的线程上</p>
<h2 id="conn-h">conn.h</h2>
<p>分别使用<code>TcpConn</code>记录访问的tcp连接，<code>TcpServer</code>(包含事件分发器)处理具体的事件</p>
<blockquote>
<p><code>enable_shared_from_this&lt;&gt;</code> 和 <code>shared_from_this()</code></p>
<p>从<code>enable_shared_from_this</code>派生的对象可以使用成员函数中的 <code>shared_from_this</code> 方法创建一个 <code>shared_ptr</code> 所有者，与现有 <code>shared_ptr</code> 所有者共享实例所有权。<br>
否则，如果通过 <code>this</code> 创建新的 <code>shared_ptr</code> ，则它与现有的 <code>shared_ptr</code> 所有者不同，这可能导致无效引用或导致该对象被删除多次。</p>
</blockquote>
<p>先来看看<code>TcpConn</code>类的一些成员函数：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//用Channel封装套接字 并 设置回调函数</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">TcpConn::attach</span><span class="params">(EventBase *base, <span class="type">int</span> fd, Ip4Addr local, Ip4Addr peer)</span></span>;</span><br><span class="line"><span class="comment">//建立非阻塞socket的tcp连接</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">TcpConn::connect</span><span class="params">(EventBase *base, <span class="type">const</span> string &amp;host, <span class="type">unsigned</span> <span class="type">short</span> port, <span class="type">int</span> timeout, <span class="type">const</span> string &amp;localip)</span></span>;</span><br><span class="line"><span class="comment">//将连接放到EventsImp的等待重连队列中，超时之后从队列中取出，重新connnect()</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">TcpConn::reconnect</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">//设置重连时间间隔，-1: 不重连，0:立即重连，其它：等待毫秒数，未设置不重连</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setReconnectInterval</span><span class="params">(<span class="type">int</span> milli)</span></span>;</span><br><span class="line"><span class="comment">//通过EventBase::safeCall()关闭channel_</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">TcpConn::close</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//表示当前连接失败或关闭</span></span><br><span class="line"><span class="comment">//(如果设置)重连 / 移出空闲监控 -&gt; 清除回调函数 -&gt; delete channel</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">TcpConn::cleanup</span><span class="params">(<span class="type">const</span> TcpConnPtr &amp;con)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//判断当前状态是否正确(connected)</span></span><br><span class="line"><span class="comment">//然后调用read()/write()</span></span><br><span class="line"><span class="comment">//根据返回的错误码执行相应的操作</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">TcpConn::handleRead</span><span class="params">(<span class="type">const</span> TcpConnPtr &amp;con)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">TcpConn::handleWrite</span><span class="params">(<span class="type">const</span> TcpConnPtr &amp;con)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//当接收到消息且处于handshaking状态就需要调用该函数，使连接监听读/写-&gt;只监听读</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">TcpConn::handleHandshake</span><span class="params">(<span class="type">const</span> TcpConnPtr &amp;con)</span>；</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="comment">//调用wirte()往socket写入数据，对不同的errno进行处理</span></span></span><br><span class="line"><span class="function"><span class="comment">//EINTR -&gt; 重新write()   </span></span></span><br><span class="line"><span class="function"><span class="comment">//EAGAIN or EWOULDBLOCK -&gt; 不继续发送，启用写事件监听，HandleWrite()处理</span></span></span><br><span class="line"><span class="function"><span class="type">ssize_t</span> <span class="title">TcpConn::isend</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *buf, <span class="type">size_t</span> len)</span></span>;</span><br><span class="line"><span class="comment">//根据参数重载，实质调用isend()发送，如果未发送完成，添加到输出缓冲区，通过handleWrite()发送</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">TcpConn::send</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//两种模式: onRead()和onMsg()只能选择其一</span></span><br><span class="line"><span class="comment">//onMsg()也是通过调用onRead()实现，只是需要先decode</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">TcpConn::onMsg</span><span class="params">(CodecBase *codec, <span class="type">const</span> MsgCallBack &amp;cb)</span></span>;</span><br><span class="line"><span class="comment">//先encode再send()</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">TcpConn::sendMsg</span><span class="params">(Slice msg)</span></span>;</span><br><span class="line">    </span><br><span class="line"><span class="comment">//连接空闲idle秒后执行回调cb</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">TcpConn::addIdleCB</span><span class="params">(<span class="type">int</span> idle, <span class="type">const</span> TcpCallBack &amp;cb)</span></span>;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>输入/输出缓冲区的设置：</p>
<p>输入/输出缓冲区的设置：</p>
<p><a href="http://www.cppblog.com/Solstice/archive/2011/04/17/144378.html">为什么 non-blocking 网络编程中应用层 buffer 是必须的？—— 陈硕的Blog</a></p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">TcpServer</span> : <span class="keyword">private</span> noncopyable &#123;</span><br><span class="line">    <span class="built_in">TcpServer</span>(EventBases *bases);</span><br><span class="line">    <span class="comment">// return 0 on sucess, errno on error</span></span><br><span class="line">    <span class="comment">//创建socket,为socket设置SO_REUSEADDR/SO_REUSEPORT/FD_CLOEXEC</span></span><br><span class="line">    <span class="comment">//通过调用::bind绑定ip和端口</span></span><br><span class="line">    <span class="comment">//lsiten()设置该套接字等待处理的队列长度为20</span></span><br><span class="line">    <span class="comment">//将套接字封装为listen_channel_，设置套接字接收数据时的执行的函数</span></span><br><span class="line">    <span class="comment">//poller::loop_once()-&gt;Channel::handleRead()-&gt;TcpServer::handleAccept()</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">bind</span><span class="params">(<span class="type">const</span> std::string &amp;host, <span class="type">unsigned</span> <span class="type">short</span> port, <span class="type">bool</span> reusePort = <span class="literal">false</span>)</span></span>;</span><br><span class="line">    <span class="comment">//创建以bases为事件分发器的tcpserver,调用TcpServer::bind()</span></span><br><span class="line">    <span class="function"><span class="type">static</span> TcpServerPtr <span class="title">startServer</span><span class="params">(EventBases *bases, <span class="type">const</span> std::string &amp;host, <span class="type">unsigned</span> <span class="type">short</span> port, <span class="type">bool</span> reusePort = <span class="literal">false</span>)</span></span>;</span><br><span class="line">    ~<span class="built_in">TcpServer</span>() &#123; <span class="keyword">delete</span> listen_channel_; &#125;</span><br><span class="line">    <span class="function">Ip4Addr <span class="title">getAddr</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> addr_; &#125;</span><br><span class="line">    <span class="function">EventBase *<span class="title">getBase</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> base_; &#125;</span><br><span class="line">    <span class="comment">//添加tcp连接时,为新的tcp连接所设置的回调函数</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">onConnCreate</span><span class="params">(<span class="type">const</span> std::function&lt;TcpConnPtr()&gt; &amp;cb)</span> </span>&#123; createcb_ = cb; &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">onConnState</span><span class="params">(<span class="type">const</span> TcpCallBack &amp;cb)</span> </span>&#123; statecb_ = cb; &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">onConnRead</span><span class="params">(<span class="type">const</span> TcpCallBack &amp;cb)</span> </span>&#123;</span><br><span class="line">        readcb_ = cb;</span><br><span class="line">        <span class="built_in">assert</span>(!msgcb_);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 消息处理与Read回调冲突，只能调用一个</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">onConnMsg</span><span class="params">(CodecBase *codec, <span class="type">const</span> MsgCallBack &amp;cb)</span> </span>&#123;</span><br><span class="line">        codec_.<span class="built_in">reset</span>(codec);</span><br><span class="line">        msgcb_ = cb;</span><br><span class="line">        <span class="built_in">assert</span>(!readcb_);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">private</span>:</span><br><span class="line">    EventBase *base_;<span class="comment">//当前服务器所使用的事件分发器</span></span><br><span class="line">    EventBases *bases_;<span class="comment">//负责为新连接分配事件分发器</span></span><br><span class="line">    Ip4Addr addr_;</span><br><span class="line">    Channel *listen_channel_;<span class="comment">//listen的套接字的Channel封装</span></span><br><span class="line">    <span class="comment">//创建的tcp新连接的回调函数</span></span><br><span class="line">    TcpCallBack statecb_, readcb_;</span><br><span class="line">    MsgCallBack msgcb_;</span><br><span class="line">    std::function&lt;TcpConnPtr()&gt; createcb_;</span><br><span class="line">    std::unique_ptr&lt;CodecBase&gt; codec_;</span><br><span class="line">    <span class="comment">//循环调用accept()当前等待处理的队列</span></span><br><span class="line">    <span class="comment">//如果当前连接和分配给新连接的事件分发器一致，则直接创建新连接</span></span><br><span class="line">    <span class="comment">//如果不一致,则通过safeCall调用创建新连接</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">handleAccept</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>还有一个半同步半异步服务器HSHA，利用线程池，为每一个消息处理都分配一个worker线程，即同步处理请求，异步管理IO</p>
<h2 id="udp-h">udp.h</h2>
<p>参考conn.h</p>
<h2 id="http-h">http.h</h2>
<p>HTTP消息是服务器和客户端之间交换数据的方式。有两种类型的消息：客户端发送的用于触发服务器上某个操作的<strong>请求</strong>和来自服务器的<strong>响应</strong>。</p>
<p><img src="httpRequest.png" alt="HTTP-Request"><img src="httpResponse.png" alt="HTTP-Response"></p>
<p>这里对这两种消息类型进行了抽象<code>HttpMsg</code>和实现<code>HttpRequest</code> / <code>HttpResponse</code></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">HttpMsg</span> &#123;</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">Result</span> &#123;Error, Complete, NotComplete, Continue100,&#125;;</span><br><span class="line">    <span class="built_in">HttpMsg</span>() &#123; HttpMsg::<span class="built_in">clear</span>(); &#125;;</span><br><span class="line">    <span class="comment">//内容添加到buf，返回写入的字节数</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">int</span> <span class="title">encode</span><span class="params">(Buffer &amp;buf)</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//尝试从buf中解析，默认复制body内容</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> Result <span class="title">tryDecode</span><span class="params">(Slice buf, <span class="type">bool</span> copyBody = <span class="literal">true</span>)</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//清空消息相关的字段</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">clear</span><span class="params">()</span></span>;</span><br><span class="line">	</span><br><span class="line">    <span class="function">std::string <span class="title">getHeader</span><span class="params">(<span class="type">const</span> std::string &amp;n)</span> </span>&#123; <span class="keyword">return</span> <span class="built_in">getValueFromMap_</span>(headers, n); &#125;</span><br><span class="line">    <span class="function">Slice <span class="title">getBody</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> body<span class="number">2.</span><span class="built_in">size</span>() ? body2 : (Slice) body; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//如果tryDecode返回Complete，则返回已解析的字节数</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">getByte</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> scanned_; &#125;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">//保存Status line/Headers/Body</span></span><br><span class="line">    std::map&lt;std::string, std::string&gt; headers;</span><br><span class="line">    std::string version, body;</span><br><span class="line">    <span class="comment">// body可能较大，为了避免数据复制，加入body2</span></span><br><span class="line">    Slice body2;</span><br><span class="line">    </span><br><span class="line">   <span class="keyword">protected</span>:</span><br><span class="line">    <span class="type">bool</span> complete_;<span class="comment">//是否完整读完一个消息</span></span><br><span class="line">    <span class="type">size_t</span> contentLen_;<span class="comment">//消息主体长度</span></span><br><span class="line">    <span class="type">size_t</span> scanned_;<span class="comment">//已经读过的长度</span></span><br><span class="line">    <span class="comment">//line1存储第一行</span></span><br><span class="line">    <span class="comment">//以第一个\r\n\r\n作为header和body的分割，如未找到，则header未读完，返回NotComplete</span></span><br><span class="line">    <span class="comment">//循环读入每一行的，以第一个词(保证跟着&quot;:&quot;)为key, 后面为value保存到headers中</span></span><br><span class="line">    <span class="comment">//最后根据headers中的&quot;content-length&quot;判断是否读完整个Http Message，没有则返回Continue100</span></span><br><span class="line">    <span class="comment">//如果选择copyBody,则将消息主体拷贝到body中，否则仅仅将指针存放在body2中，返回Complete </span></span><br><span class="line">    <span class="function">Result <span class="title">tryDecode_</span><span class="params">(Slice buf, <span class="type">bool</span> copyBody, Slice *line1)</span></span>;</span><br><span class="line">    <span class="function">std::string <span class="title">getValueFromMap_</span><span class="params">(std::map&lt;std::string, std::string&gt; &amp;m, <span class="type">const</span> std::string &amp;n)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>HTTP Request Target <a href="https://tools.ietf.org/html/rfc7230#page-41">Request Target</a> / <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Messages">HTTP Message - MDN web docs</a></p>
<p>HTTP Request Target <a href="https://tools.ietf.org/html/rfc7230#page-41">Request Target</a> / <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Messages">HTTP Message - MDN web docs</a></p>
<ul>
<li>
<p>origin-form</p>
<p>absolute-path [ “?” query ]</p>
<p>the most common form, and it is used with <code>GET</code>, <code>POST</code>, <code>HEAD</code>, and <code>OPTIONS</code> methods.</p>
<p>e.g. GET /background.png HTTP/1.0 HEAD /test.html?query=alibaba HTTP/1.1</p>
</li>
<li>
<p>absolute-form</p>
<p>absolute-URI</p>
<p>e.g. GET <a href="http://developer.mozilla.org/en-US/docs/Web/HTTP/Messages">http://developer.mozilla.org/en-US/docs/Web/HTTP/Messages</a> HTTP/1.1</p>
</li>
<li>
<p>authority-form</p>
<p>The authority component of a URL, consisting of the domain name and optionally the port (prefixed by a ‘:’), is called the authority form. It is only used with CONNECT when setting up an HTTP tunnel.</p>
<p>e.g. CONNECT <a href="http://developer.mozilla.org/">developer.mozilla.org:80</a> HTTP/1.1</p>
</li>
<li>
<p>asterisk-form</p>
<p>Used with <code>OPTIONS</code>, representing the server as a whole.</p>
<p>e.g. OPTIONS * HTTP/1.1</p>
</li>
</ul>
</blockquote>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">HttpRequest</span> : <span class="keyword">public</span> HttpMsg &#123;</span><br><span class="line">    <span class="built_in">HttpRequest</span>() &#123; <span class="built_in">clear</span>(); &#125;</span><br><span class="line">    std::map&lt;std::string, std::string&gt; args;</span><br><span class="line">    <span class="comment">//保存HTTP method(GET/PUT/POST/HEAD...)</span></span><br><span class="line">    <span class="comment">//query_uri保存完整的request target</span></span><br><span class="line">    <span class="comment">//而uri则保存其中的绝对路径,在请求中与查询字符串以?分隔</span></span><br><span class="line">    std::string method, uri, query_uri;</span><br><span class="line">    <span class="function">std::string <span class="title">getArg</span><span class="params">(<span class="type">const</span> std::string &amp;n)</span> </span>&#123; <span class="keyword">return</span> <span class="built_in">getValueFromMap_</span>(args, n); &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// override</span></span><br><span class="line">    <span class="comment">// 按格式将Start line/headers(手动添加content-length和长连接)/body添加到buf中</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">int</span> <span class="title">encode</span><span class="params">(Buffer &amp;buf)</span></span>;</span><br><span class="line">    <span class="comment">//只支持origin-form的解析</span></span><br><span class="line">    <span class="comment">//将query_uri中&quot;?&quot;后面的查询字符串，通过查找&quot;&amp;&quot;和&quot;=&quot;,提取query参数到args中</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> Result <span class="title">tryDecode</span><span class="params">(Slice buf, <span class="type">bool</span> copyBody = <span class="literal">true</span>)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        HttpMsg::<span class="built_in">clear</span>();</span><br><span class="line">        args.<span class="built_in">clear</span>();</span><br><span class="line">        method = <span class="string">&quot;GET&quot;</span>;</span><br><span class="line">        query_uri = uri = <span class="string">&quot;&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><code>HttpResponse</code>类和<code>HttpRequest</code>只有以下区别（不考虑HTTP1.1和2的区别(chunked)）：</p>
<ul>
<li>
<p>HTTP response的起始行叫做<code>status line</code>，包含协议版本/状态码/状态的简单描述</p>
<p>e.g. HTTP/1.1 404 Not Found</p>
</li>
<li>
<p>与Request Header相对应的是Response Header，提供有关服务器的其他信息</p>
<p>e.g. Vary / Accept-Ranges</p>
</li>
</ul>
<p>所以解码只需通过<code>HttpMsg</code>继承的<code>tryDecode_(Slice buf, bool copyBody, Slice *line1)</code>完成，然后单独处理<code>line1</code></p>
<p><code>HttpConnPtr</code>类是本质上是一条Tcp连接，进一步封装主要是加入了HttpRequest，HttpResponse的处理</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">HttpConnPtr</span> &#123;</span><br><span class="line">    TcpConnPtr tcp;</span><br><span class="line">    <span class="built_in">HttpConnPtr</span>(<span class="type">const</span> TcpConnPtr &amp;con) : <span class="built_in">tcp</span>(con) &#123;&#125;</span><br><span class="line">    <span class="comment">//允许将HttpConnPtr转换成TcpConnPtr,就是获得其中的tcp</span></span><br><span class="line">    <span class="function"><span class="keyword">operator</span> <span class="title">TcpConnPtr</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> tcp; &#125;</span><br><span class="line">    <span class="comment">//重载-&gt;，返回shared_ptr&lt;TcpCoon&gt;中存储的指针</span></span><br><span class="line">    TcpConn *<span class="keyword">operator</span>-&gt;() <span class="type">const</span> &#123; <span class="keyword">return</span> tcp.<span class="built_in">get</span>(); &#125;</span><br><span class="line">    <span class="type">bool</span> <span class="keyword">operator</span>&lt;(<span class="type">const</span> HttpConnPtr &amp;con) <span class="type">const</span> &#123; <span class="keyword">return</span> tcp &lt; con.tcp; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">typedef</span> std::function&lt;<span class="type">void</span>(<span class="type">const</span> HttpConnPtr &amp;)&gt; HttpCallBack;</span><br><span class="line">	<span class="comment">//利用AutoContext类实现HttpContext的内存管理 并 提供访问</span></span><br><span class="line">    <span class="function">HttpRequest &amp;<span class="title">getRequest</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> tcp-&gt;internalCtx_.<span class="built_in">context</span>&lt;HttpContext&gt;().req; &#125;</span><br><span class="line">    <span class="function">HttpResponse &amp;<span class="title">getResponse</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> tcp-&gt;internalCtx_.<span class="built_in">context</span>&lt;HttpContext&gt;().resp; &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">sendRequest</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="built_in">sendRequest</span>(<span class="built_in">getRequest</span>()); &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">sendResponse</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="built_in">sendResponse</span>(<span class="built_in">getResponse</span>()); &#125;</span><br><span class="line">    <span class="comment">//将http request编码后放到tcp的output缓冲区进行发送</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">sendRequest</span><span class="params">(HttpRequest &amp;req)</span> <span class="type">const</span> </span>&#123;&#125;</span><br><span class="line">    <span class="comment">//将http response编码后放到tcp的output缓冲区进行发送</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">sendResponse</span><span class="params">(HttpResponse &amp;resp)</span> <span class="type">const</span> </span>&#123;&#125;</span><br><span class="line">    <span class="comment">//文件作为Response</span></span><br><span class="line">    <span class="comment">//先读取文件内容，然后将内容指针存到getResponse()的body2,sendResponse()</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">sendFile</span><span class="params">(<span class="type">const</span> std::string &amp;filename)</span> <span class="type">const</span></span>;</span><br><span class="line">    <span class="comment">//客户端:将tcp的输入缓冲区中已解析为HttpResponse的数据移除,然后清空getResponse()</span></span><br><span class="line">    <span class="comment">//服务端:将tcp的输入缓冲区中已解析为HttpRequest的数据移除,然后清空getRequest()</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">clearData</span><span class="params">()</span> <span class="type">const</span></span>;</span><br><span class="line">	<span class="comment">//设置tcp数据到达时的回调函数:</span></span><br><span class="line">    <span class="comment">//为tcp生成一个HttpConnPtr对象从而使用其中的handleRead()</span></span><br><span class="line">    <span class="comment">//将tcp缓冲区中解析到getRequest()/getResponse(),并执行回调函数cb()</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">onHttpMsg</span><span class="params">(<span class="type">const</span> HttpCallBack &amp;cb)</span> <span class="type">const</span></span>;</span><br><span class="line">    <span class="comment">//当数据到达</span></span><br><span class="line">    <span class="comment">//poller::loop_once()-&gt;Channel::handleRead()-&gt;TcpConn::handleRead()</span></span><br><span class="line">    <span class="comment">//-&gt;TcpConn::readcb_()</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">protected</span>:</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">HttpContext</span> &#123;</span><br><span class="line">        HttpRequest req;</span><br><span class="line">        HttpResponse resp;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="comment">//如果是服务器,从tcp的输入缓冲区解析到getRequest()</span></span><br><span class="line">    <span class="comment">//解析出错，则关闭tcp连接,解析未完成则请求客户端继续发送,解析完成执行回调函数</span></span><br><span class="line">    <span class="comment">//如果是客户端,从tcp的输入缓冲区解析到getResponse()</span></span><br><span class="line">    <span class="comment">//解析出错，则关闭tcp连接,解析完成执行回调函数</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">handleRead</span><span class="params">(<span class="type">const</span> HttpCallBack &amp;cb)</span> <span class="type">const</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">logOutput</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *title)</span> <span class="type">const</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">HttpServer</span> : <span class="keyword">public</span> TcpServer &#123;</span><br><span class="line">    <span class="comment">//TcpServer(bases)</span></span><br><span class="line">    <span class="comment">//设置当前tcp连接accept其他connect请求时,新的tcp的连接方式:</span></span><br><span class="line">    <span class="comment">//创建一条新的tcp连接,用它生成一个HttpConnPtr,设置该HttpConnPtr的onHttpMsg():</span></span><br><span class="line">    <span class="comment">//当HttpConnPtr所拥有的tcp数据到来时,解析为httpRequest,并执行对应的cbs_,再执行defcb_</span></span><br><span class="line">    <span class="built_in">HttpServer</span>(EventBases *bases);</span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">Conn</span> = TcpConn&gt;</span><br><span class="line">    <span class="type">void</span> <span class="built_in">setConnType</span>() &#123;</span><br><span class="line">        conncb_ = [] &#123; <span class="keyword">return</span> <span class="built_in">TcpConnPtr</span>(<span class="keyword">new</span> Conn); &#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//设置当接收到GET uri的请求时,需要执行的函数</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">onGet</span><span class="params">(<span class="type">const</span> std::string &amp;uri, <span class="type">const</span> HttpCallBack &amp;cb)</span> </span>&#123; cbs_[<span class="string">&quot;GET&quot;</span>][uri] = cb; &#125;</span><br><span class="line">    <span class="comment">//设置当接收到何种方法,何种uri的请求时需要执行的函数</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">onRequest</span><span class="params">(<span class="type">const</span> std::string &amp;method, <span class="type">const</span> std::string &amp;uri, <span class="type">const</span> HttpCallBack &amp;cb)</span> </span>&#123; cbs_[method][uri] = cb; &#125;</span><br><span class="line">    <span class="comment">//设置接收并以对应的方式处理完之后还需要执行的函数</span></span><br><span class="line">    <span class="comment">//默认是发送404 Not Found的Http Response</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">onDefault</span><span class="params">(<span class="type">const</span> HttpCallBack &amp;cb)</span> </span>&#123; defcb_ = cb; &#125;</span><br><span class="line">   <span class="keyword">private</span>:</span><br><span class="line">    HttpCallBack defcb_;	<span class="comment">//处理完请求之后的回调函数</span></span><br><span class="line">    std::function&lt;TcpConnPtr()&gt; conncb_;</span><br><span class="line">    std::map&lt;std::string, std::map&lt;std::string, HttpCallBack&gt;&gt; cbs_;</span><br><span class="line">    <span class="comment">//保存[method][uri]对应请求所需执行的函数</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="conf-h">conf.h</h2>
<p><code>Conf</code>类是对INI配置文件读写的封装</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 0 success</span></span><br><span class="line"><span class="comment">// -1 IOERROR</span></span><br><span class="line"><span class="comment">// &gt;0 line no of error</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">parse</span><span class="params">(<span class="type">const</span> std::string &amp;filename)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get a string value from INI file</span></span><br><span class="line"><span class="function">std::string <span class="title">get</span><span class="params">(std::string section, std::string name, std::string default_value)</span></span>;</span><br><span class="line"><span class="comment">// Get an integer (long) value from INI file</span></span><br><span class="line"><span class="function"><span class="type">long</span> <span class="title">getInteger</span><span class="params">(std::string section, std::string name, <span class="type">long</span> default_value)</span></span>;</span><br><span class="line"><span class="comment">// Get a real (floating point double) value from INI file</span></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">getReal</span><span class="params">(std::string section, std::string name, <span class="type">double</span> default_value)</span></span>;</span><br><span class="line"><span class="comment">// Get a boolean value from INI file</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">getBoolean</span><span class="params">(std::string section, std::string name, <span class="type">bool</span> default_value)</span></span>;</span><br><span class="line"><span class="comment">// Get a string value from INI file</span></span><br><span class="line"><span class="function">std::list&lt;std::string&gt; <span class="title">getStrings</span><span class="params">(std::string section, std::string name)</span></span>;</span><br><span class="line"></span><br><span class="line">std::map&lt;std::string, std::list&lt;std::string&gt;&gt; values_;</span><br><span class="line">std::string filename;</span><br></pre></td></tr></table></figure>
<h2 id="daemon-h">daemon.h</h2>
<blockquote>
<p>参考<a href="http://www.enderunix.org/docs/eng/daemon.php">Unix Daemon Server Programming</a></p>
</blockquote>
<p>创建守护进程一般有以下几步</p>
<ol>
<li>
<p><code>fork()</code>父进程，让父进程退出，则孤儿进程会成为 <code>init</code> 的子进程</p>
</li>
<li>
<p>调用<code>setsid()</code>创建新会话，当前进程成为 新会话&amp;新进程组 的领导进程，没有控制tty</p>
<blockquote>
<p>在Unix系统中，进程在进程组内运行，因此在一个进程组中的所有进程被视为单个实体，子进程也会继承父进程的进程组和会话。而服务器不应从启动它的进程中接收信号，因此它必须将自己与控制tty分离，所以我们需要通过<code>setsid()</code>进一步独立进程</p>
</blockquote>
</li>
<li>
<p>大多数服务器以超级用户身份运行，出于安全原因，它们应保护自己创建的文件。<code>umask()</code> 设置用户掩码将防止在创建文件时可能出现的不安全文件特权。</p>
</li>
<li>
<p>服务器应在已知目录中运行 <code>chdir(&quot;/servers/&quot;)</code></p>
</li>
<li>
<p>大多数服务一次只需要运行一个服务器副本。因此我们通过文件锁定<code>loackf</code>来保持单例。服务器的第一个实例将锁定文件，以便其他实例了解实例已在运行。 如果服务器终止，锁将自动释放，以便新实例可以运行。</p>
</li>
<li>
<p>进程可能会从用户或进程接收信号，因此最好抓住这些信号并相应地执行操作。 子进程终止时会发送SIGCHLD信号，服务器进程必须忽略或处理这些信号。</p>
</li>
<li>
<p>日志消息的处理 -&gt; 标准IO / 写入文件 / 系统日志守护进程syslogd</p>
</li>
</ol>
<h2 id="stat-svr-h">stat-svr.h</h2>
<p>对<code>HttpServer</code>添加query 参数stat，方便外部的工具查看应用程序的状态</p>
<h2 id="总结">总结</h2>
<p><code>EventBases</code>负责线程与<code>EventBase</code>（持有<code>poller</code>）之间的联系，底层的<code>poller</code>（使用epoll水平触发） 检测到<code>POLLIN</code> / <code>POLLOUT</code>读写事件时，调用事件对应 <code>Channel</code> 的 <code>handleRead()</code> / <code>handleWrite()</code> 执行 I/O，具体的操作取决于上层的协议，如<code>TcpCoon</code> 所持有的套接字的Channel的就会调用<code>TcpConn::handleRead()</code>，然后一次性把 socket 里的数据读完（从操作系统 buffer 搬到应用层 buffer），否则会反复触发 <code>POLLIN</code> 事件，造成 <code>busy-loop</code> 。对于数据不完整的情况，则数据依然存放在应用层buffer中直到构成完整的消息，因此每个 <code>TcpConn</code> 都需配置 <code>input buffer</code>，同理也需要<code>output buffer</code>，并且程序只负责调用<code>send</code>往应用层buffer中添加发送内容，然后由库接手，为 <code>socket</code> 注册 <code>POLLOUT</code> 事件，只要<code>socket</code>可写就将数据从应用层buffer搬到操作系统buffer，直到写完后停止<code>POLLOUT</code></p>
<p><img src="reactor.png" alt="Reactor"></p>
<p>线程模型：</p>
<p>每个线程最多有一个 <code>EventBase</code>，每个<code>TcpConn</code> 所在的线程由其所属的 <code>EventBase</code> （<code>TcpServer</code>通过调用<code>allocBase()</code>分配）决定</p>
<p>所有对IO和buffer的读写，都应该在IO线程中完成</p>
<p>单线程：同步处理请求，同步管理IO，全部在主线程完成，不需要考虑线程安全</p>
<p>多线程：</p>
<ul>
<li>
<p>MultiBase —— 通过vector + atomic&lt;int&gt;实现对 <code>EventBase</code> 的分配（Round Robin）</p>
<p>（类似于多个单线程模型，只是需要考虑线程安全）</p>
<p>一个reactor对应一个<code>EventBase</code>。主Reactor只有一个，只负责监听新的连接，accept后将这个连接分配到子Reactor上，子Reactor可以有多个，这样可以分摊一个<code>EventBase</code>的压力。</p>
<p><img src="reactor-master.png" alt="reactor-master"></p>
</li>
<li>
<p>HSHA服务器（同步处理请求，异步管理IO）则主要是实现了上图下部的<code>Thread Pool</code>的例子，可以设置消息模式时，将每一个读写任务添加到queued tasks中即业务线程池，适用于阻塞型或者耗时型的任务，尽可能不影响每一个<code>EventBase::loop()</code></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Programing</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>Linux</tag>
        <tag>Socket</tag>
      </tags>
  </entry>
  <entry>
    <title>Interprocess Communications</title>
    <url>/2020/08/25/interprocess-communication/</url>
    <content><![CDATA[<h2 id="消息传递">消息传递</h2>
<h3 id="管道">管道</h3>
<blockquote>
<p>匿名管道随进程的创建而建立，随进程的结束而销毁</p>
</blockquote>
<p>管道是基于Linux &amp; Unix的操作系统的核心概念之一。管道允许您以一种非常优雅的方式 <code>|</code> 将命令链接在一起，将一个程序的输出传递为另一个程序的输入。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ls</span> -la | <span class="built_in">sort</span> | less</span><br></pre></td></tr></table></figure>
<p><code>bash</code> 的执行过程</p>
<ol>
<li><code>bash</code> 创建2个管道，一个从 <code>ls</code> 到 <code>sort</code>，一个从 <code>sort</code> 到 <code>less</code></li>
<li><code>bash</code> <code>fork</code> 三个子进程分别用于每个命令</li>
<li>子进程1 <code>ls</code> 设置 <code>stdout</code> 为 <code>pipe A</code> 的写端</li>
<li>子进程2 <code>sort</code> 设置 <code>stdin</code> 为 <code>pipe A</code> 的读端，设置 <code>stdout</code> 为 <code>pipe B</code> 的写端</li>
<li>子进程3 <code>less</code> 设置 <code>stdin</code> 为 <code>pipe B</code> 的读端</li>
</ol>
<blockquote>
<p>Linux has a VFS (virtual file system) module called <code>pipefs</code>, that gets mounted in kernel space during boot. <code>pipefs</code> is mounted <strong>alongside the root file system</strong> (/), not in it (pipe’s root is <code>pipe:</code> ). <code>pipefs</code> is stored using an in-memory file system.</p>
<p><code>pipefs</code> cannot be directly examined by the user unlike most file systems. The entry point to <code>pipefs</code> is the <strong><code>pipe</code> syscall</strong>. The <strong><code>pipe</code> syscall</strong> is used by shells and other programs to implement piping, and just creates a new file in pipefs, returning two file descriptors (one for the read end, opening using <code>O_RDONLY</code>, and one for the write end, opened using <code>O_WRONLY</code>).</p>
<img src="./interprocess-communication.assets/pipe_impl.png" alt="impl" style="zoom:50%;" />
</blockquote>
<img src="./interprocess-communication.assets/pipe.png" alt="pipe" style="zoom:33%;" />
<p>使用 <code>fork</code> 创建子进程，<strong>创建的子进程会复制父进程的文件描述符</strong>，这样就做到了两个进程各有两个 <code>fd[0]</code> 与 <code>fd[1]</code>，两个进程就可以通过各自的 fd 写入和读取同一个管道文件实现跨进程通信。</p>
<p>父子进程间的单向数据流（半双工管道）</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> pipe_fd[<span class="number">2</span>];</span><br><span class="line">    pipe(pipe_fd);</span><br><span class="line">    </span><br><span class="line">    <span class="type">pid_t</span> childpid = fork();</span><br><span class="line">    <span class="keyword">if</span>(childpid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        close(pipe_fd[<span class="number">1</span>]);</span><br><span class="line">        <span class="comment">//close(pipe_fd[0]);</span></span><br><span class="line">        <span class="comment">// TODO</span></span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">	close(pipe_fd[<span class="number">0</span>]);</span><br><span class="line">	<span class="comment">//close(pipe_fd[1]);</span></span><br><span class="line"></span><br><span class="line">    waitpid(childpid,null,<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>父子进程间的双向数据流的两个管道（全双工管道，有两个半双工管道构成）</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> pipe_fd1[<span class="number">2</span>],pipe_fd2[<span class="number">2</span>];</span><br><span class="line">    pipe(pipe_fd1);</span><br><span class="line">    pipe(pipe_fd2);</span><br><span class="line">    </span><br><span class="line">    <span class="type">pid_t</span> childpid = fork();</span><br><span class="line">    <span class="keyword">if</span>(childpid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        close(pipe_fd1[<span class="number">1</span>]);</span><br><span class="line">        close(pipe_fd2[<span class="number">0</span>]);</span><br><span class="line">        <span class="comment">//close(pipe_fd1[0]);</span></span><br><span class="line">        <span class="comment">//close(pipe_fd2[1]);</span></span><br><span class="line">        <span class="comment">//TODO</span></span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">	close(pipe_fd1[<span class="number">0</span>]);</span><br><span class="line">	close(pipe_fd2[<span class="number">1</span>]);</span><br><span class="line">	<span class="comment">//close(pipe_fd1[1]);</span></span><br><span class="line">	<span class="comment">//close(pipe_fd2[0]);</span></span><br><span class="line">    waitpid(childpid,null,<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="FIFO">FIFO</h4>
<blockquote>
<p>名字空间</p>
<p>当两个或多个无亲缘关系的进程使用某种类型的IPC对象来彼此交换信息时，该IPC对象必须有一个某种形式的 <strong>名字(name)</strong> 或 <strong>标识符(identifier)</strong> ，这样其中一个进程（往往是服务器）可以创建该IPC对象，其余进程则可以指定同一个IPC对象</p>
</blockquote>
<p>管道没有名字，因此它们的最大劣势就是智能用于有一个共同祖先进程的各个进程之间。而 <strong>FIFO（指代first in, first out）</strong> 类似于管道（单向数据流），且有一个文件路径名与之关联从而实现无亲缘关系进程访问同一个FIFO，所以也被称为 <strong>命名管道(named pipe)</strong></p>
<blockquote>
<p>Any FIFO is much like a pipe: rather than owning disk blocks in the filesystems, an opened FIFO is associated with a kernel buffer that temporarily stores the data exchanged by two or more processes. Thanks to the disk inode, however, a FIFO can be accessed by any process, since the FIFO filename is included in the system’s directory tree. FIFO inodes <strong>appear on the system directory tree rather than on the pipefs special filesystem</strong>.</p>
<img src="./interprocess-communication.assets/fifo.png" alt="impl" style="zoom:50%;" />
<p>Allocates a <code>pipe_inode_info</code> object (the actual pipe) for providing buffer space when fifo_open(…)</p>
<img src="./interprocess-communication.assets/linux_pipe.png" alt="impl" style="zoom:50%;" />
</blockquote>
<p>通过 <code>mkfifo</code> 命令来创建并指定管道名字</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">mkfifo myFifo</span><br><span class="line"><span class="comment">//Example: write first</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">char</span> * myfifo = <span class="string">&quot;/tmp/myfifo&quot;</span>; </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Creating the named file(FIFO) </span></span><br><span class="line">    <span class="comment">// mkfifo(&lt;pathname&gt;, &lt;permission&gt;) </span></span><br><span class="line">    mkfifo(myfifo, <span class="number">0666</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="type">char</span> arr1[<span class="number">80</span>], arr2[<span class="number">80</span>]; </span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) </span><br><span class="line">    &#123; </span><br><span class="line">        <span class="comment">// Open FIFO for write only </span></span><br><span class="line">        fd = open(myfifo, O_WRONLY); </span><br><span class="line">  </span><br><span class="line">        <span class="comment">// Take an input arr2ing from user. </span></span><br><span class="line">        <span class="comment">// 80 is maximum length </span></span><br><span class="line">        fgets(arr2, <span class="number">80</span>, <span class="built_in">stdin</span>); </span><br><span class="line">  </span><br><span class="line">        <span class="comment">// Write the input arr2ing on FIFO </span></span><br><span class="line">        <span class="comment">// and close it </span></span><br><span class="line">        write(fd, arr2, <span class="built_in">strlen</span>(arr2)+<span class="number">1</span>); </span><br><span class="line">        close(fd); </span><br><span class="line">  </span><br><span class="line">        <span class="comment">// Open FIFO for Read only </span></span><br><span class="line">        fd = open(myfifo, O_RDONLY); </span><br><span class="line">  </span><br><span class="line">        <span class="comment">// Read from FIFO </span></span><br><span class="line">        read(fd, arr1, <span class="keyword">sizeof</span>(arr1)); </span><br><span class="line">  </span><br><span class="line">        <span class="comment">// Print the read message </span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;User2: %s\n&quot;</span>, arr1); </span><br><span class="line">        close(fd); </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="消息队列">消息队列</h3>
<blockquote>
<p>随内核的持续性</p>
</blockquote>
<p>消息队列可认为时一个<strong>消息链表</strong>，有足够写权限的线程可往队列中放置消息，有足够读权限的线程可从队列中取走消息。</p>
<p>每个消息都具有如下属性：</p>
<ul>
<li>一个无符号整数优先级（Posix） / 一个长整数类型（System V）</li>
<li>消息的数据部分长度</li>
<li>数据本身</li>
</ul>
<h4 id="System-V-消息队列">System V 消息队列</h4>
<blockquote>
<img src="./interprocess-communication.assets/SystemVmq.png" alt="impl" style="zoom:50%;" />
</blockquote>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/ipc.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/msg.h&gt;</span></span></span><br><span class="line"><span class="comment">// structure for message queue</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">msg_buffer</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="type">long</span> msg_type;</span><br><span class="line">    <span class="type">char</span> msg[<span class="number">100</span>];</span><br><span class="line">&#125; message;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">send</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">key_t</span> my_key;</span><br><span class="line">    <span class="type">int</span> msg_id;</span><br><span class="line">    my_key = ftok(<span class="string">&quot;progfile&quot;</span>, <span class="number">65</span>);             <span class="comment">//create unique key</span></span><br><span class="line">    msg_id = msgget(my_key, <span class="number">0666</span> | IPC_CREAT); <span class="comment">//create message queue and return id</span></span><br><span class="line">    message.msg_type = <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Write Message : &quot;</span>);</span><br><span class="line">    fgets(message.msg, <span class="number">100</span>, <span class="built_in">stdin</span>);</span><br><span class="line">    msgsnd(msg_id, &amp;message, <span class="keyword">sizeof</span>(message), <span class="number">0</span>); <span class="comment">//send message</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Sent message is : %s \n&quot;</span>, message.msg);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">receive</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">key_t</span> my_key;</span><br><span class="line">    <span class="type">int</span> msg_id;</span><br><span class="line">    my_key = ftok(<span class="string">&quot;progfile&quot;</span>, <span class="number">65</span>);                   <span class="comment">//create unique key</span></span><br><span class="line">    msg_id = msgget(my_key, <span class="number">0666</span> | IPC_CREAT);       <span class="comment">//create message queue and return id</span></span><br><span class="line">    msgrcv(msg_id, &amp;message, <span class="keyword">sizeof</span>(message), <span class="number">1</span>, <span class="number">0</span>); <span class="comment">//used to receive message</span></span><br><span class="line">    <span class="comment">// display the message</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Received Message is : %s \n&quot;</span>, message.msg);</span><br><span class="line">    msgctl(msg_id, IPC_RMID, <span class="literal">NULL</span>); <span class="comment">//destroy the message queue</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Posix-消息队列">Posix 消息队列</h4>
<blockquote>
<p>POSIX message queues implement <strong>priority-ordered</strong> messages. Each message written by a sender process is associated with an integer number which is interpreted as message priority; messages with a higher number are considered higher in priority.</p>
<img src="./interprocess-communication.assets/Posixmq.png" alt="impl" style="zoom:50%;" />
</blockquote>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mqueue.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAX_SIZE 1024</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> QUEUE_NAME <span class="string">&quot;/test&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">createAndReceive</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">mqd_t</span> mq;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">mq_attr</span> <span class="title">attr</span>;</span></span><br><span class="line">    <span class="type">char</span> buffer[MAX_SIZE + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* initialize the queue attributes */</span></span><br><span class="line">    attr.mq_flags = <span class="number">0</span>;</span><br><span class="line">    attr.mq_maxmsg = <span class="number">10</span>;</span><br><span class="line">    attr.mq_msgsize = MAX_SIZE;</span><br><span class="line">    attr.mq_curmsgs = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* create the message queue */</span></span><br><span class="line">    mq = mq_open(QUEUE_NAME, O_CREAT | O_RDONLY, <span class="number">0644</span>, &amp;attr);</span><br><span class="line"></span><br><span class="line">    <span class="type">ssize_t</span> bytes_read;</span><br><span class="line">    <span class="comment">/* receive the message */</span></span><br><span class="line">    bytes_read = mq_receive(mq, buffer, MAX_SIZE, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    mq_close(mq);</span><br><span class="line">    mq_unlink(QUEUE_NAME);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">send</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">mqd_t</span> mq;</span><br><span class="line">    <span class="type">char</span> buffer[MAX_SIZE];</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* open the mail queue */</span></span><br><span class="line">    mq = mq_open(QUEUE_NAME, O_WRONLY);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">memset</span>(buffer, <span class="number">0</span>, MAX_SIZE);</span><br><span class="line">    fgets(buffer, MAX_SIZE, <span class="built_in">stdin</span>);</span><br><span class="line"></span><br><span class="line">    mq_send(mq, buffer, MAX_SIZE, <span class="number">0</span>);</span><br><span class="line">    mq_close(mq);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="区别">区别</h4>
<ul>
<li>对Posix消息队列的读总是返回最高优先级的最早消息，对SystemV消息队列的读则可以返回任意优先级的消息</li>
<li>当往一个空队列放置一个消息时，Posix消息队列允许产生一个信号或启动一个线程，SystemV消息队列则不提供类似机制</li>
</ul>
<h2 id="参考">参考</h2>
<p><a href="https://wywwwwei.github.io/2020/08/25/interprocess-communication/">Unix 网络编程 卷2: 进程间通信</a></p>
<p><a href="https://www.slideshare.net/divyekapoor/linux-kernel-implementation-of-pipes-and-fifos">Slide - Linux Kernel Implementation of Pipes FIFOs</a></p>
<p><a href="https://mp.weixin.qq.com/s/mblyh6XrLj1bCwL0Evs-Vg">进程间通信</a></p>
]]></content>
      <categories>
        <category>Concepts</category>
      </categories>
      <tags>
        <tag>Operating System</tag>
      </tags>
  </entry>
  <entry>
    <title>Notes for OSTEP</title>
    <url>/2020/08/03/ostep-1/</url>
    <content><![CDATA[<h1><em>Operating Systems: Three Easy Pieces</em></h1>
<blockquote>
<p>Notes - Part One</p>
<p>Links to the book: <a href="http://pages.cs.wisc.edu/~remzi/OSTEP/">http://pages.cs.wisc.edu/~remzi/OSTEP/</a></p>
</blockquote>
<h2 id="Introduction">Introduction</h2>
<blockquote>
<p>What an OS actually does</p>
</blockquote>
<p>It takes physical resources, such as a CPU, memory, or disk, and <strong>virtualizes</strong> them. It handles tough and tricky issues related to <strong>concurrency</strong>. And it stores files <strong>persistently</strong>, thus making them safe over the long-term.</p>
<blockquote>
<p>Design goal</p>
</blockquote>
<ul>
<li>build up some <strong>abstractions</strong> in order to make the system convenient and easy to use</li>
<li>provide high <strong>performance</strong> / <strong>minimize the overheads</strong> of the OS</li>
<li>provide <strong>protections</strong> between applications, as well as the OS and applications.</li>
<li>strive to provide a high degree of <strong>reliability</strong>. ( run non-stop )</li>
<li>energy-efficiency / security / mobility…</li>
</ul>
<blockquote>
<p>History of how OS developed</p>
</blockquote>
<ol>
<li>
<p>Just Libraries: a set of libraries of commonly-used functions</p>
<p>Batch Processing: a number of jobs were set up and then run in a “batch” by the operator</p>
</li>
<li>
<p>Protection</p>
<p>if allow any program read any file? -&gt; the code run on behalf of the OS was special -&gt; <strong>system call</strong></p>
<blockquote>
<p>Key difference between a system call and a procedure call</p>
<p>A system call transfer control into the OS while simultaneously raising the <strong>hardware privilege level</strong>. User applications run in what is referred to an <strong>user mode</strong> which means the hardware restricts what applications can do.</p>
<p>When a system call is initiated( usually through a special hardware instruction called a <strong>trap</strong>), the hardware transfers control to a pre-specified <strong>trap handler</strong>( that the OS set up previously) and simultaneously raises the privilege level to <strong>kernel mode</strong>( In kernel mode, the OS has full access to the hardware of system). When the OS is done servicing the request, it passed control back to the user via a special <strong>return-from-trap</strong> instruction, which reverts to user mode while simultaneously passing control back to where the application let off.</p>
</blockquote>
</li>
<li>
<p>Era of Multiprogramming</p>
<p>desire to make better use of machine resources -&gt;</p>
<p><strong>multiprogramming</strong>: load a number of jobs into memory and switch rapidly between them for improving CPU utilization -&gt;</p>
<p>don’t want one program to be able to access the memory of another program -&gt;</p>
<p><strong>memory protection</strong> &amp; other <strong>concurrency</strong> issues</p>
</li>
<li>
<p>Modern Era</p>
</li>
</ol>
<h2 id="Virtualization">Virtualization</h2>
<h3 id="The-Abstraction-The-Process">The Abstraction: The Process</h3>
<p>The definition of a process -&gt; ( informally ) a running program</p>
<p>A program -&gt; a lifeless thing: sits on the disk, a bunch of instructions and maybe some static data, waiting to spring into action</p>
<p><img src="process.png" alt="process"></p>
<blockquote>
<p>Summarize a process by taking an inventory of the different pieces of the system it accesses or affects</p>
</blockquote>
<ul>
<li>
<p>the memory that the process can address</p>
<p>Instructions / the data it reads or writes sits in memory</p>
</li>
<li>
<p>registers、</p>
<p>Many instructions explicitly read or update registers. (includes some special registers like IP/SP…)</p>
</li>
<li>
<p>access persistent storage devices</p>
<p>Might include a list of the files it currently has open.</p>
</li>
</ul>
<blockquote>
<p>Process creation</p>
</blockquote>
<ol>
<li>
<p>load its code and any static data (e.g., initialized variables) into memory, into the address space of the process</p>
<p>from eager (Early OS: <strong>all at once</strong> before running) to lazy (Modern OS: <strong>load pieces</strong> when needed during execution)</p>
</li>
<li>
<p>allocate memory for program’s <strong>run-time stack</strong></p>
</li>
<li>
<p>allocate memory for program’s <strong>heap</strong></p>
</li>
<li>
<p>other work as related to <strong>I/O setup</strong></p>
<p>e.g., in Unix systems, each process by default has 3 open file descriptors for standard input/output/error.</p>
</li>
</ol>
<blockquote>
<p>Process state in a simplified view</p>
</blockquote>
<p><img src="process_state.png" alt="process state"></p>
<ul>
<li>running: runing on a processor and executing instructions.</li>
<li>ready: ready to run but for some reason the OS has chosen not to run it at this given moment.</li>
<li>blocked: has performed some kind of operation that makes it not ready to run until some other event takes place</li>
</ul>
<p>other state:</p>
<ul>
<li>initial: begin created</li>
<li>final: has exited but has not yet been cleaned up (zombie state)</li>
</ul>
<blockquote>
<p>Key data structures to track information</p>
</blockquote>
<p>track state of each process -&gt; process list</p>
<p>individual structure stores information about a process -&gt; process control block:</p>
<img src="pcb.png" alt="PCB" style="zoom:33%;" />
<blockquote>
<p>How can the OS provide the illusion of a nearly-endless supply of CPUs</p>
</blockquote>
<p>By <strong>virtualizing</strong> the CPU. By running one process, then stopping it and running another, and so forth, it can promote the illusion.</p>
<p><strong>Mechanisms</strong></p>
<blockquote>
<p>low-level methods or protocols that implement a needed piece of functionality. -&gt; how</p>
</blockquote>
<ul>
<li>
<p>Time sharing</p>
<p>It is a basic technique used by an OS to share a resource. By allowing the resource to be used for a little while by one entity, and then a little while by another, and so forth, the resources in question can be shared by many.</p>
</li>
<li>
<p>Context switch</p>
<p>Give the OS the ability to stop running one program and start running another on a given CPU.</p>
</li>
</ul>
<p><strong>Policies</strong></p>
<blockquote>
<p>On top of the mechanisms (high-level), algorithms for making some kind of decision -&gt; which</p>
</blockquote>
<ul>
<li>
<p>scheduling policy</p>
<p>Determine which program should the OS run.</p>
</li>
</ul>
<h3 id="Key-Process-API">Key Process API</h3>
<ul>
<li>The <strong>fork()</strong> system call is used in Unix systems to create a new process. The creator is called the <strong>parent</strong>; the newly created process is called the <strong>child</strong>.</li>
<li>The <strong>wait()</strong> system call allows a parent to wait for its child to complex execution.</li>
<li>The <strong>exec()</strong> family of system calls allows a child to break free from its similarity to its parent and execute an entirely new program.</li>
</ul>
<blockquote>
<p>Why the separation of <strong>fork()</strong> and <strong>exec()</strong> is essential in building a Unix shell</p>
</blockquote>
<p>It lets the shell run code <em>after</em> the call to <strong>fork()</strong> but <em>before</em> the call to <strong>exec()</strong>; this code can alter the environment of the about-to-be-run program, and thus enables a variety of interesting features to be readily built.</p>
<h3 id="Mechanism-Limited-Direct-Execution">Mechanism: Limited Direct Execution</h3>
<blockquote>
<p>Challenges</p>
</blockquote>
<ul>
<li>performance - implement virtualization without adding excessive overhead to system</li>
<li>control - run process efficiently while retaining control over the CPU</li>
</ul>
<blockquote>
<p>Direct Execution Protocol (Without Limits)</p>
</blockquote>
<ol>
<li>
<p>OS:</p>
<p>Create entry for process list -&gt; Allocate memory for program -&gt; Load program into memory -&gt; Set up stack with argc / argv -&gt; Clear registers -&gt; Execute call main()</p>
</li>
<li>
<p>Program:</p>
<p>Run main -&gt; Execute return from main</p>
</li>
<li>
<p>OS:</p>
<p>Free memory of process -&gt; Remove from process list</p>
</li>
</ol>
<p>Advantage: fast</p>
<p>Problems:</p>
<ol>
<li>
<p>Restricted Operations</p>
<blockquote>
<p>A process must be able to perform I/O and some other restricted operations, but without giving the process complete control over the system.</p>
</blockquote>
<p>Approach: a new process mode</p>
<p>Code that runs in <strong>user mode</strong> is <strong>restricted</strong> in what it can do. In contrast to user mode is <strong>kernel mode</strong>, which the operating system (or kernel) runs in. In this mode, code that runs can <strong>do what it likes</strong>, including privileged operations.</p>
<p>To make a user process perform some kind of privileged operation when it wishes, the <strong>system call</strong> is provided. To execute a system call, a program must execute a special <strong>trap</strong> instruction. This instruction simultaneously jumps into the kernel and raises the privilege level to <strong>kernel mode</strong>; once in kernel, the system can now perform whatever privileged operations are needed, and thus <strong>do the required work for the calling process</strong>. When finished, the OS calls a special <strong>return-from-trap</strong> instruction to return into the calling user program while simultaneously reducing the privilege level <strong>back to user mode</strong>.</p>
<p>In order to be able to return correctly when the OS issues the return-from-trap instruction, it must make sure to <strong>save enough of the caller’s registers</strong>. For example, on x86, the processor will push them onto <strong>per-process kernel stack</strong> and the return-from-trap will pop them off and resume execution of the user-mode process.</p>
<blockquote>
<p>How does the trap know which code to run inside the OS</p>
</blockquote>
<p>Clearly, if the calling process does this by <strong>specifying an address to jump to</strong>, it will allow programs to jump anywhere into the kernel which is a <strong>very bad idea</strong>.</p>
<p>The kernel does so by setting up a trap table at boot time. When the machine boot up, it does so in privileged (kernel) mode, and thus is free to configure machine hardware as need be.</p>
<p>One of the first things the OS thus does is to tell the hardware what code to run (<strong>location of trap handlers</strong>) when certain exceptional events occur. Once the hardware is informed, it remembers the location of these handlers until the machine is <strong>next rebooted</strong>.</p>
<p>To specify the exact system call, a <strong>system-call number</strong> is usually assigned to each system call. The user code is thus responsible for placing the desired system-call number in a <strong>register</strong> or at a specified location on the <strong>stack</strong>; the OS,when handling the system call inside the trap handler, <strong>examines this number</strong>, ensures it is valid, and if it is, executes the corresponding code.</p>
<p>One last aside: being able to execute the instruction to tell the hardware <strong>where the trap tables are</strong> is also a <strong>privileged operation</strong>.</p>
<blockquote>
<p>Limited Direct Execution Protocol (two phases)</p>
</blockquote>
<p>In the first (at boot time), the kernel initializes the trap table, and the CPU remembers its location for subsequent use. The kernel does so via a privileged instruction (highlighted).</p>
<img src="lde_1.png" alt="lde1" style="zoom:60%;" />
<p>In the second (when running a process), <strong>the kernel sets up</strong> a few things such as allocation before using a return-from-trap instruction to start the execution of the process; this <strong>switches the CPU to user mode</strong> and begins running the process. When the process wishes to <strong>issue a system call</strong>, it <strong>traps back</strong> into the OS, which <strong>handles</strong> it and once again <strong>return control via a return-from-trap</strong> to the process. The process then completes its work, and returns from main(); this usually will return into some stub code which will properly <strong>exit the program (by calling the exit() system call)</strong>. At this point, the OS cleans up.</p>
<img src="lde_2.png" alt="lde" style="zoom:60%;" />
</li>
<li>
<p>Switching Between Process</p>
<blockquote>
<p>If a process is running on the CPU, this by definition means the OS is not running. So how can the operating system <strong>regain control</strong> of the CPU so that it can switch between process?</p>
</blockquote>
<p><strong>A cooperative approach: wait for system calls</strong></p>
<p>In this style, the OS <strong>trusts the processes</strong> of the system to <strong>behave reasonably</strong>. Processes that run for too long are assumed to periodically give up the CPU so that the OS can decide to run some other task.</p>
<p>In a cooperative scheduling system, the OS regains control of the CPU by <strong>waiting for a system call or an illegal operation</strong> of some kind to take place. This passive approach is too ideal.</p>
<p><strong>A non-cooperative approach: the OS takes control</strong></p>
<p>Without some additional help from the hardware, it turns out that the OS can’t do much at all when a process refuses to make system calls (or mistakes) and thus return control to the OS.</p>
<p>The answer to how to gain control <strong>without cooperation</strong> is simple: a <strong>timer interrupt</strong>. A timer device can be programmed to raise an interrupt every so many milliseconds; when the interrupt is raised,the currently running process is <strong>halted</strong>, and a <strong>pre-configured interrupt handler</strong> in the OS runs. At this point, the OS has regained control of the CPU, and thus can do what it pleases: stop the current process, and start a different one.</p>
<img src="timer.png" alt="timer" style="zoom:60%;" />
<blockquote>
<p>Saving and Restoring Context</p>
</blockquote>
<p>It is made by a part of the operating system known as the scheduler to decide whether to continue running the currently-running process, or switch to a different one.</p>
<p>If the decision is made to switch, the OS then executes a low-level piece of code which we refer to as a <strong>context switch</strong>. A context switch is conceptually simple: all the OS has to do is <strong>save a few register values</strong> for the currently-running process (onto its kernel stack, for example), and <strong>restore a few</strong> (from its kernel stack), and <strong>switch to the kernel stack</strong> for the soon-to-be-executing process. By switching stacks, the kernel enters the call to the switch code in the context of one process (the one that was interrupted) and returns in the context of another (the soon-to-be-executing one). When the OS finally executes a return-from-trap instructions, the soon-to-be-executing process becomes the currently-running process. And thus the context switch is complete.</p>
<img src="switch.png" alt="context switch" style="zoom:60%;" />
<p>two types of register saves/restores</p>
<ol>
<li>When <strong>interrupt occurs</strong>, the <strong>user registers</strong> of the running process are implicitly saved by the <strong>hardware</strong>, using the <strong>kernel stack</strong> of that process.</li>
<li>When the OS <strong>decides to switch</strong>, the <strong>kernel registers</strong> are explicitly saved by the <strong>software</strong> (i.e., the OS) , into memory in the <strong>process structure</strong> of the process.</li>
</ol>
</li>
</ol>
<blockquote>
<p>What happen if, during interrupt or trap handling, another interrupt occurs. (Detailed discussion is defer until second piece)</p>
</blockquote>
<p>One simple thing an OS might do is <strong>disable interrupts</strong> during interrupt processing; doing so ensures that when one interrupt is being handled, no other one will be delivered to the CPU. But disabling interrupts for too long could <strong>lead to lost interrupts</strong>.</p>
<p>Operating systems also have developed a number of <strong>sophisticated locking schemes</strong> to protect concurrent access to internal data structures. This enables <strong>multiple activities to be on-going</strong> within the kernel at the same time, particularly useful on multiprocessors.</p>
]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Operating System</tag>
      </tags>
  </entry>
  <entry>
    <title>Notes for OSTEP</title>
    <url>/2020/09/10/ostep-2/</url>
    <content><![CDATA[<h1><em>Operating Systems: Three Easy Pieces</em></h1>
<blockquote>
<p>Notes - Part Two</p>
<p>Links to the book: <a href="http://pages.cs.wisc.edu/~remzi/OSTEP/">http://pages.cs.wisc.edu/~remzi/OSTEP/</a></p>
</blockquote>
<h2 id="Virtualization">Virtualization</h2>
<h3 id="Scheduling-Introduction">Scheduling: Introduction</h3>
<blockquote>
<p>How to develop a basic framework for thinking about scheduling policies</p>
</blockquote>
<ul>
<li>
<p>Workload Assumptions</p>
<p>Before getting into the range of possible policies, let us first make a number of <strong>simplifying assumptions</strong> about the process running in the system, sometimes collectively called <strong>workload</strong>.</p>
<ol>
<li>Each job runs for the same amount of time</li>
<li>All jobs arrive at the same time</li>
<li>Once started, each job runs to completion</li>
<li>All jobs only use the CPU (i.e., they perform no I/O)</li>
<li>The run-time of each job is known</li>
</ol>
<p>The workload assumptions are mostly <strong>unrealistic</strong>, but that is alright (for now), because we will <strong>relax</strong> them as we go, and eventually develop what we will refer to as a <strong>fully-operational scheduling discipline</strong>.</p>
</li>
<li>
<p>Scheduling Metrics</p>
<p>A metric is just something that we use to measure something. Here we simply use a single metric: <strong>turnaround time</strong>. The turnaround time of a job is defined as the time at which job completes minus the time at which the job arrived in the system.</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>T</mi><mrow><mi>t</mi><mi>u</mi><mi>r</mi><mi>n</mi><mi>a</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi></mrow></msub><mo>=</mo><msub><mi>T</mi><mrow><mi>c</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></msub><mo>−</mo><msub><mi>T</mi><mrow><mi>a</mi><mi>r</mi><mi>r</mi><mi>i</mi><mi>v</mi><mi>a</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{turnaround}=T_{completion}−T_{arrival}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">na</span><span class="mord mathnormal mtight">ro</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">co</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">pl</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">rr</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>It is a <strong>performance metric</strong>, and another metric of interest is <strong>fairness</strong>, as measured (for example) by <strong>Jain’s Fairness Index</strong>. Performance and fairness are often <strong>at odds</strong> in scheduling.</p>
</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">FIFO</th>
<th style="text-align:left">SJF</th>
<th style="text-align:left">STCF / PSJF / SRTF</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">non-preemptive</td>
<td style="text-align:left">non-preemptive</td>
<td style="text-align:left">preemptive</td>
</tr>
<tr>
<td style="text-align:left">Principle</td>
<td style="text-align:left">run the jobs that arrives earlier</td>
<td style="text-align:left">runs the shortest job first</td>
<td style="text-align:left">runs the jobs has the least time left</td>
</tr>
<tr>
<td style="text-align:left">Timing</td>
<td style="text-align:left">a job complete its burst time or switches to waiting state</td>
<td style="text-align:left">a job complete its burst time or switches to waiting state</td>
<td style="text-align:left">any time a new job arrives (switch to ready state)</td>
</tr>
<tr>
<td style="text-align:left">Problem</td>
<td style="text-align:left"><a href="https://www.geeksforgeeks.org/convoy-effect-operating-systems/">convoy effect</a></td>
<td style="text-align:left">convoy effect</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">Solve</td>
<td style="text-align:left"></td>
<td style="text-align:left">jobs that run for different amounts of time</td>
<td style="text-align:left">convoy effect</td>
</tr>
</tbody>
</table>
<hr>
<blockquote>
<p>A new metric for those demand interactive performance</p>
</blockquote>
<p>Now users would sit at a terminal and <strong>demand interactive performance</strong> from the system as well. And thus, a new metric was born: <strong>response time</strong>. We define response time as from when the job arrives in a system to the first time it is scheduled.</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>T</mi><mrow><mi>r</mi><mi>e</mi><mi>s</mi><mi>p</mi><mi>o</mi><mi>n</mi><mi>s</mi><mi>e</mi></mrow></msub><mo>=</mo><msub><mi>T</mi><mrow><mi>f</mi><mi>i</mi><mi>r</mi><mi>s</mi><mi>t</mi><mi>r</mi><mi>u</mi><mi>n</mi></mrow></msub><mo>−</mo><msub><mi>T</mi><mrow><mi>a</mi><mi>r</mi><mi>r</mi><mi>i</mi><mi>v</mi><mi>a</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{response}=T_{firstrun}−T_{arrival}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">res</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">se</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">rs</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">rr</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>But STCF and related disciplines are <strong>not particularly good for response time</strong>. For example, if three jobs arrive at the same time, the third job has to wait for the previous two jobs to run in their entirely before being scheduled just once.</p>
<p>The general technique of <strong>amortization</strong> is commonly used in systems when there is a fixed cost of some operation. So a new scheduling algorithm, <strong>Round-Robin</strong> scheduling, is proposed. The basic idea is simple, instead of running jobs to completion, RR runs a job for a <strong>time slice</strong> (<strong>scheduling quantum</strong>) and then switch to the <strong>next job in the run queue</strong>.</p>
<p>The length of the time slice is critical for RR, making it <strong>long enough</strong> to amortize the cost of switching without making it so long that the system is no longer responsive.</p>
<blockquote>
<p>Note that the cost of context switching does not arise solely from the <strong>OS actions of saving and restoring a few registers</strong>. When program runs, they build up <strong>a great deal of state in CPU caches, TLBs, branch prediction, and other on-chip hardware</strong>. Switching to another job causes this state to be flushed and new state relevant to the currently-running job to be brought in, which may exact <strong>a noticeable performance cost</strong>.</p>
</blockquote>
<p>RR, with a reasonable time slice, is thus an <strong>excellent</strong> scheduler if <strong>response time</strong> is the only metric. But it is indeed one of the <strong>worst policies</strong> if <strong>turnaround time</strong> is the metric.</p>
<p>An inherent trade-off:</p>
<p>If you are willing to unfair, you can run shortest jobs to completion, but at the cost of response time;</p>
<p>if you instead value fairness, response time is lower, but at the cost of turnaround time.</p>
<hr>
<blockquote>
<p>Relax Assumption 4: all programs perform I/O.</p>
</blockquote>
<p>A scheduler clearly has a decision to make both when a job initiates an I/O request and when the I/O completes.</p>
<p>The scheduler incorporate I/O by <strong>treating each CPU burst as a job</strong>, the scheduler makes sure process that are “interactive” get <strong>run frequently</strong>. While those interactive jobs are performing I/O, other <strong>CPU-intensive</strong> jobs run, thus better utilizing the processor.</p>
<p>Take the STCF scheduler for example, both A and B need 50ms CPU time, but A runs for 10ms and then issues an I/O request whereas B performs no I/O.</p>
<table>
<thead>
<tr>
<th style="text-align:center">Treating A as a single job</th>
<th style="text-align:center">Treating each CPU burst of A as a job</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="sub_job_1.png" alt="before"></td>
<td style="text-align:center"><img src="sub_job_2.png" alt="after"></td>
</tr>
</tbody>
</table>
<hr>
<blockquote>
<p>Relax Assumption 5:</p>
<p>In a general-purpose OS, it usually knows very <strong>little about the length of each job</strong>. So how can we build an approach that behaves like SJF/STCF <strong>without such a priori knowledge</strong>?</p>
</blockquote>
<h3 id="Scheduling-The-Multi-Level-Feedback-Queue">Scheduling: The Multi-Level Feedback Queue</h3>
<blockquote>
<p>Basic Rule</p>
</blockquote>
<p>The MLFQ has a number of distinct queues, each assigned a different <strong>priority level</strong>. At any time, a job that is ready to run is on a single queue.</p>
<p>Basic principle of decision:</p>
<ul>
<li>Rule 1: if Priority(A) &gt; Priority(B), A runs (B doesn’t)</li>
<li>Rule 2: if Priority(A) = Priority(B), A &amp; B run in RR</li>
</ul>
<p>The key to MLFQ scheduling therefore lies in how the scheduler sets priorities. Rather than giving a fixed priority to each job, MLFQ <strong>varies the priority of a job based on its observed behavior</strong>. For example,</p>
<p>a job repeatedly relinquishes the CPU -&gt; (might be) interactive -&gt; priority high</p>
<p>a job uses the CPU intensively -&gt; reduce its priority</p>
<blockquote>
<p>How to change priority</p>
</blockquote>
<p>First attempt at a priority adjustment algorithm:</p>
<ul>
<li>
<p>Rule 3: when a job enters the system, it is placed at the highest priority (the topmost queue)</p>
</li>
<li>
<p>Rule 4a: if a job uses up an entire time slice while running, its priority is reduced.</p>
</li>
<li>
<p>Rule 4b: if a job gives up the CPU before the time slice is up, it stays at the same priority level.</p>
<blockquote>
<p>Problems with current MLFQ</p>
</blockquote>
<ol>
<li>
<p>Starvation</p>
<p>if there are “too many” interactive jobs in the system, they will combine to consume all CPU time, and thus long-running jobs will never receive any CPU time (they starve).</p>
</li>
<li>
<p>A smart user could rewrite their program to <strong>game the scheduler</strong> which refers to the idea of doing something sneaky to <strong>trick</strong> the scheduler into giving you <strong>more than your fair share of the resource</strong>.</p>
</li>
<li>
<p>A program may change its behavior over time; what was CPU-bound may transition to a phase of interactivity.</p>
</li>
</ol>
</li>
</ul>
<blockquote>
<p>The priority boost to avoid starvation</p>
</blockquote>
<ul>
<li>Rule 5: After some time period S, move all the jobs in the system to the topmost queue.</li>
</ul>
<p>In fact, this new rule solves two problems at once. First, processes are <strong>guaranteed not to starve</strong>. Second, if a CPU-bound job has <strong>become interactive</strong>, the scheduler <strong>treats it properly</strong> once it has <strong>received the priority boost</strong>.</p>
<p>Of course, the addition of the time period S leads to the obvious question: what should S be set to? If it is set too high, long-running jobs could starve; too slow, and interactive jobs may not get a proper share of the CPU.</p>
<blockquote>
<p>Better accounting to avoid gaming of scheduler</p>
</blockquote>
<p>The real culprit are rules 4a and 4b, which let a job <strong>retain its priority by relinquishing</strong> the CPU before the time slice expires.</p>
<p>Rewrite them to be the following single rule:</p>
<ul>
<li>Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced.</li>
</ul>
<h4 id="Summary">Summary</h4>
<p>As for how to parameterize a scheduler, there are <strong>no easy answers</strong> to these questions, and thus only some <strong>experience with workloads and subsequent tuning</strong> of the scheduler will lead to a satisfactory balance.</p>
<img src="MLFQ.png" alt="MLFQ" style="zoom:60%;" />
<h3 id="Scheduling-Proportional-Share">Scheduling: Proportional Share</h3>
<p>Instead of optimizing for turnaround time or response time, a scheduler might instead try to guarantee that each job obtain a certain percentage of CPU time.</p>
<blockquote>
<p>How to share the CPU proportionally?</p>
</blockquote>
<p>Not Deterministic - Lottery scheduling</p>
<ol>
<li>
<p>Use of randomness</p>
<p>It achieves this probabilistically (but not deterministically) by holding a lottery every time slice. Holding a lottery is straightforward: the scheduler must know how many total tickets there are. The scheduler then picks a winning ticket to determine which process to run.</p>
<p>For example: Process A holds tickets 0 through 74 and process B holds tickets 75 through 99</p>
<p>Then the scheduler choose from 0 - 99 and loads the state of the winning process.</p>
<p>The <strong>randomness</strong> in lottery scheduling leads to <strong>a probabilistic correctness in meeting the desire proportion, but no guarantee</strong>.</p>
</li>
<li>
<p>Ticket currency</p>
<p>Currency allows a user with a set of tickets to <strong>allocate tickets among their jobs</strong> in whatever currency they would like; the system automatically converts said currency into the <strong>correct global value</strong>.</p>
</li>
<li>
<p>Ticket transfer</p>
<p>With transfers, a process can <strong>temporarily hand off its tickets</strong> to another process and it is especially useful in a client/server setting.</p>
</li>
<li>
<p>Ticket inflation</p>
<p>With inflation, a process can <strong>temporarily raise or lower the number of tickets it owns</strong>. (Apply in an environment where a group of processes trust one another)</p>
</li>
</ol>
<p>But the ticket-assignment problem remains open.</p>
<p>While randomness gets us a simple scheduler, it occasionally will not deliver the exact right proportions, especially over short time scales.</p>
<hr>
<p>Deterministic - Stride scheduling</p>
<p>Each job in the system has a <strong>stride</strong>, which is <strong>inverse</strong> in proportion to the number of tickets it has. Every time a process runs, we will increment a counter for it (called its <strong>pass value</strong>) by its stride to track its global progress. Then pick the process to run that has the lowest pass value so far.</p>
<p>pick client with min pass -&gt; run for quantum -&gt; update pass using stride -&gt; return the client to queue</p>
<blockquote>
<p>Disadvantage compared with lottery scheduling</p>
</blockquote>
<p>It owns a <strong>global state</strong>.</p>
<p>Imagine a new job enters in the middle of our stride scheduling, <strong>what should its pass value be</strong>? If it is set to 0, it will monopolize the CPU.</p>
<p>With lottery scheduling, there is no global state per process; we simply add a new process with whatever tickets it has, update the single global variable to track <strong>how many total tickets we have</strong>, and go from there.</p>
<hr>
<p>Linux Completely Fair Scheduler</p>
<p>Goal: to fairly divide a CPU evenly among all competing processes.</p>
<p>It does so through a simple <strong>counting-based</strong> technique known as <strong>virtual runtime</strong>. As each process runs, it <strong>accumulates vruntime</strong>. When a scheduling decision occurs, CFS will pick the process with the <strong>lowest vruntime to run next</strong>.</p>
<blockquote>
<p>How does the scheduler know when to stop the currently running process, and run the next one?</p>
</blockquote>
<p>If CFS switches too often, fairness is increased, as CFS will ensure that each process receives its share of CPU even over miniscule time windows, but at the cost of performance (too much context switching).</p>
<p>If CFS switches less often, performance is increased (reduced context switching), but at cost of near-term fairness.</p>
<blockquote>
<p>CFS manage the tension above through various control parameters</p>
</blockquote>
<ul>
<li>
<p><code>sched_latency</code> (A typical value is 48ms)</p>
<p>Determine how long one process should run before considering a switch (effectively determining its time slice but in a dynamic fashion). <strong>CFS divides this value by the number (n) of processes running on the CPU to determine the time slice for a process</strong>, and thus ensures that over this period of time, CFS will be completely fair.</p>
<p>If there are too many processes running, it leads to too small of a time slice and thus too many context switches.</p>
</li>
<li>
<p><code>min_granularity</code> (usually set to a value like 6ms)</p>
<p>CFS will <strong>never set the time slice of a process to less than this value</strong>, ensuring that not too much time is spent in scheduling overhead. Although CFS won’t be perfectly fair over the target scheduling latency, it will be close, while still achieving high CPU efficiency.</p>
</li>
</ul>
<p>CFS tracks vruntime precisely which allows a job has a time slice that is not a perfect multiple of the time interrupt interval.</p>
<blockquote>
<p>Weighting design (niceness)</p>
</blockquote>
<p>The <strong>nice</strong> level of a process. (can be set anywhere from -20 to +19 with a default of 0. Positive nice value imply lower priority and negative values imply higher priority)</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi mathvariant="normal">_</mi><mi>s</mi><mi>l</mi><mi>i</mi><mi>c</mi><msub><mi>e</mi><mi>k</mi></msub><mo>=</mo><mfrac><mrow><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><msub><mi>t</mi><mi>k</mi></msub></mrow><mrow><munderover><mo>∑</mo><mn>0</mn><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></munderover><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><msub><mi>t</mi><mi>i</mi></msub><mo>∗</mo><mi>s</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>y</mi></mrow></mfrac><mspace linebreak="newline"></mspace><mi>v</mi><mi>r</mi><mi>u</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>m</mi><msub><mi>e</mi><mi>i</mi></msub><mo>=</mo><mi>v</mi><mi>r</mi><mi>u</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>m</mi><msub><mi>e</mi><mi>i</mi></msub><mo>+</mo><mfrac><mrow><mi>N</mi><mi>I</mi><mi>C</mi><mi>E</mi><mi mathvariant="normal">_</mi><mi>O</mi><mi mathvariant="normal">_</mi><mi>L</mi><mi>O</mi><mi>A</mi><mi>D</mi></mrow><mrow><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><msub><mi>t</mi><mi>i</mi></msub></mrow></mfrac><mo>∗</mo><mi>r</mi><mi>u</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>m</mi><msub><mi>e</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">time\_slice_k=\frac{weight_k}{\sum_{0}^{n-1}weight_i*sched\_latency}\\
vruntime_i=vruntime_i+\frac{NICE\_O\_LOAD}{weight_i}∗runtime_i
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">im</span><span class="mord mathnormal">e</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal">c</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.5254em;vertical-align:-1.154em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.156em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.954em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">sc</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.03588em;">cy</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.154em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">im</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">im</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.2638em;vertical-align:-0.8804em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3833em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.7em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal" style="margin-right:0.05764em;">CE</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">im</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>NICE_O_LOAD is a constant default weight of a process.</p>
<blockquote>
<p>Using Red-Black Trees</p>
<p>To find the next job to run quickly as soon as possible</p>
</blockquote>
<p>CFS only keep running (or runnable) processes in this structure.</p>
<p>Insert / delete job: O( log(n) )</p>
<blockquote>
<p>Dealing with I/O and Sleeping processes</p>
</blockquote>
<p>CFS handles them by <strong>altering the vruntime of a job when it wakes up</strong>. Specially, CFS sets the vruntime of that job to the <strong>minimum value</strong> found in the tree to avoid starvation of other processes.</p>
<h4 id="Summary-2">Summary</h4>
<p>No scheduler is a panacea, and fair-share schedulers have their fair share of problems.</p>
<ol>
<li>do not particularly mesh well with I/O</li>
<li>ticket or priority assignment problem</li>
</ol>
<h3 id="Multiprocessor-Scheduling">Multiprocessor Scheduling</h3>
<blockquote>
<p>difference between single-CPU hardware and multi-CPU hardware</p>
</blockquote>
<p>The use of hardware caches and exactly how data is shared across multiple processors.</p>
<p>Caches fast memories that <strong>hold copies of popular data</strong> that is found in the <strong>main memory</strong> of the system.</p>
<p>Caches are based on the <strong>notion of locality</strong>.</p>
<ul>
<li>
<p>Temporal locality</p>
<p>When a piece of data is accessed, it is likely to be accessed again in the near future</p>
</li>
<li>
<p>Spatial locality</p>
<p>If a program accesses a data item at address <em>x</em>, it is likely to access data item near <em>x</em> as well.</p>
</li>
</ul>
<p>Caching with multiple CPUs is much more complicated. For example, it will cause a general problem - <strong>cache coherence</strong></p>
<blockquote>
<p>Solution to maintain a cache coherence</p>
</blockquote>
<p>Basic solution provided by the hardware: by <strong>monitoring memory accesses</strong>, hardware can ensure that basically the <strong>“right thing” happens</strong> and that the view of a single shared memory is preserved.</p>
<p>One way to do this on a bus-based system is to use an old technique known as <strong>bus snooping</strong>; each cache pays attention to memory updates by observing the bus that connects them to main memory. When a CPU then sees an update for a data item it holds in memory, it will <strong>notice the change</strong> and either <strong>invalidate</strong> (remove it from its own cache) its copy or <strong>update</strong> it (put the new value into its cache).</p>
<blockquote>
<p>Synchronization</p>
</blockquote>
<p>When accessing / updating shared data items or structures across CPUs, <strong>mutual exclusion primitives</strong> (such as <strong>locks</strong>) should likely to be used to guarantee correctness (other approaches, such as <strong>building lock-free data structures</strong>). But locking will create the problem with regards to performance especially as the number of CPU grows.</p>
<blockquote>
<p>Cache affinity</p>
</blockquote>
<p>A process, when run on a particular CPU, builds up a fair bit of state in the caches (and TLBs) of the CPU. The next time the process runs, it is often advantageous to run it <strong>on the same CPU</strong>, as it will run faster if <strong>some of its state is already present</strong> in the caches on that CPU.</p>
<p>A multiprocessor scheduler should consider cache affinity when making its scheduling decisions, perhaps <strong>preferring to keep a process on the same CPU</strong> if all at possible.</p>
<h4 id="Single-Queue-Scheduling">Single-Queue Scheduling</h4>
<p>The most basic approach is to simply reuse the basic framework for single processor scheduling, by <strong>putting all jobs that need to scheduled into a single queue</strong>; we call this single-queue multiprocessor scheduling (SQMS).</p>
<p>Advantage:</p>
<ul>
<li>Simplicity. Not require much work to take an existing pick policy and adapt it to work on more than one CPU.</li>
</ul>
<p>Disadvantage:</p>
<ul>
<li>
<p>Lack of scalability. The developers have to inserted some form of locking into code which greatly reduce performance to ensure correct access to the single queue.</p>
</li>
<li>
<p>Cache affinity</p>
<p>Each CPU simply picks the next job to run from the globally shared queue, each job ends up bouncing around from CPU to CPU, thus doing exactly the opposite of what would make sense from the stand point of cache affinity.</p>
<p>To handle this problem, most SQMS schedulers include some kind of affinity mechanism. One might provide affinity for some jobs, but move others around to balance load.</p>
</li>
</ul>
<h4 id="Multi-Queue-Scheduling">Multi-Queue Scheduling</h4>
<p>Some system opt for multiple queues, e.g., one per CPU. We call it multi-queue multiprocessor scheduling (MQMS)</p>
<p>In MQMS, our basic scheduling framework consists of multiple scheduling queues. <strong>Each queue will likely follow a particular scheduling discipline</strong>, such as round-robin, though of any algorithm can be used.</p>
<p>When a job enters the system, it is placed on exactly one scheduling queue, according to some heuristic. Then it is scheduled essentially independently.</p>
<p>Advantage:</p>
<ul>
<li>Avoiding the problems of information sharing and synchronization found in the single-queue approach</li>
<li>Inherently more scalable ( lock and cache contention not become a central problem; intrinsically provides cache affinity)</li>
</ul>
<p>Disadvantage:</p>
<ul>
<li><strong>load imbalance</strong></li>
</ul>
<blockquote>
<p>How to deal with load imbalance</p>
</blockquote>
<p>By <strong>migrating</strong> a job from one CPU to another, true load balance can be achieved.</p>
<blockquote>
<p>How should the system decide to enact a migration</p>
</blockquote>
<p><strong>Work stealing</strong>. A queue that is low on jobs will occasionally peek at another queue, to see how full it is. If the target queue is more full than the source queue, the source will steal one or more jobs from the target to help balance load.</p>
<p>However, if you look around at other queues too often, you will suffer from <strong>high overhead and have trouble scaling</strong>. If you don’t look at other queues very often, you are in danger of <strong>suffering from severe load imbalances</strong>.</p>
<h4 id="Linux-Multiprocessor-Schedulers">Linux Multiprocessor Schedulers</h4>
<p>Both O(1) and CFS use multiple queues, whereas BFS uses a single queue.</p>
<p>O(1) scheduler is a priority-based scheduler (similar to the MLFQ), changing a process’s priority over time and then scheduling those with highest priority in order to meet various scheduling objectives.</p>
<p>CFS, in contrast, is a deterministic proportional-share approach (more like stride scheduling).</p>
<p>BFS, is also proportional-share, but based on a more complicated scheme known as Earliest Eligible Virtual Deadline First (EEVDF).</p>
]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Operating System</tag>
      </tags>
  </entry>
  <entry>
    <title>Notes for OSTEP</title>
    <url>/2021/08/23/ostep-3/</url>
    <content><![CDATA[<h1><em>Operating Systems: Three Easy Pieces</em></h1>
<blockquote>
<p>Notes - Part Three</p>
<p>Links to the book: <a href="http://pages.cs.wisc.edu/~remzi/OSTEP/">http://pages.cs.wisc.edu/~remzi/OSTEP/</a></p>
</blockquote>
<h2 id="Memory-Virtualization">Memory Virtualization</h2>
<h3 id="Abstraction-Address-Space">Abstraction: Address Space</h3>
<p>While saving and restoring register-level state is relatively fast, saving the entire contents of memory to disk is brutally non-performant. Thus, we’d rather do is leave processes in memory while switching between them, allowing the OS to implement time sharing efficiently. As new demands (isolation / protection) were placed on the operating system, we need to create an easy to use <strong>abstraction</strong> of physical memory. We call this abstraction the <strong>address space</strong>.</p>
<p>The address space of a process contains all of the memory state of the running program.</p>
<ol>
<li>code of the program have to live in memory somewhere</li>
<li>while it is running, uses a stack to keep track of where it is in the function call chain as well as to allocate local variables and pass parameters and return values to and from routines.</li>
<li>heap is used for dynamically-allocated, user-managed memory.</li>
</ol>
<img src="address_space.png" alt="address space" style="zoom:50%;" />
<p>However, this placement of stack and heap is just a convention; you could arrange the address space in a different way if you’d like; as we will see later, when multiple threads co-exist in an address space, no nice way to divide the address space like this works anymore.</p>
<blockquote>
<p>Goal</p>
</blockquote>
<ol>
<li>
<p>Transparency.</p>
<p>The OS should implement virtual memory in a way that is invisible to the running program.</p>
</li>
<li>
<p>Efficiency</p>
<p>Make the virtualization as efficient as possible both in terms of time and space.</p>
</li>
<li>
<p>Protection</p>
<p>The OS should make sure to protect process from one another as well as the OS itself from processes.</p>
</li>
</ol>
<h3 id="Memory-API">Memory API</h3>
<blockquote>
<p>Underlying OS Support</p>
<p>The malloc library manages space within your virtual address space, but itself is built on top of some system calls which call into the OS to ask for more memory or release some back to system.</p>
</blockquote>
<ul>
<li>
<p><code>brk</code></p>
<p>Used to change the location of the program’s break: the location of the end of the heap.</p>
<p>It takes one argument (the address of the new break), and thus either increases or decreases the size of the heap based on whether the new break is larger or smaller than the current break</p>
</li>
<li>
<p><code>mmap</code></p>
<p>Obtain memory from the operating system via the <code>mmap()</code> call</p>
<p>It can create an anonymous memory region within your program - a region which is not associated with any particular file but rather with <strong>swap space</strong>. This memory can then also be treated like a heap</p>
</li>
</ul>
<h3 id="Mechanism-Address-Translation">Mechanism: Address Translation</h3>
<ul>
<li><strong>Efficiency</strong> dictates that we make use of hardware support, which at first will be quiet rudimentary (e.g. just a few registers) but will grow to be fairly complex (e.g. TLBs, page-table support, and so forth).</li>
<li><strong>Control</strong> implies that the OS ensures that no application is allowed to access any memory but its own; thus, to protect applications from one another, and the OS from applications, we will need help from the hardware.</li>
</ul>
<p>With <strong>hardware-based address translation</strong>, the <strong>hardware transform</strong> each memory access, changing the virtual address provided by the instruction to a physical address where the desired information is actually located.</p>
<p>But the hardware just provides the low-level mechanism for virtualize memory efficiently. The <strong>OS</strong> must get involved at key point to set up hardware so that the correct translation take place; it must <strong>manage</strong> memory, keeping track of which locations are free and which are in use, and judiciously intervening to maintain control over how memory is used.</p>
<blockquote>
<p>Assumptions</p>
</blockquote>
<ol>
<li>The user’s address space must be place contiguously in physical memory.</li>
<li>It is less than the size of physical memory.</li>
<li>Each address space is exactly the same size.</li>
</ol>
<blockquote>
<p>Dynamic (Hardware-based) Relocation</p>
</blockquote>
<p>Two hardware registers within each CPU: one is called the base register, and the other the bounds/limit register. Sometimes people call the part of the processor that helps with address translation the <strong>memory management unit (MMU)</strong>.</p>
<ul>
<li>A small aside about bound register, which can be defined in one of two ways
<ol>
<li>It holds the size of the address space and thus the hardware checks the virtual address against it first before adding the base</li>
<li>It holds the physical address of the end of the address space, and thus the hardware first adds the base and then makes sure the address is within bounds.</li>
</ol>
</li>
</ul>
<p>Each memory reference generated by the process is a virtual address, the hardware in turn adds the contents of the base register to this address and the result is a physical address that can be issued to the memory system.</p>
<blockquote>
<p>Hardware support</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:left">Hardware Requirements</th>
<th style="text-align:left">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Privileged mode</td>
<td style="text-align:left">Needed to prevent user-mode processes from executing privileged operations</td>
</tr>
<tr>
<td style="text-align:left">Base/bounds registers</td>
<td style="text-align:left">Need pair of register per CPU to support address translation and bounds checks</td>
</tr>
<tr>
<td style="text-align:left">Ability to translate virtual addresses and check if within bounds</td>
<td style="text-align:left">Circuitry to do translations and check limits; in this case, quiet simple</td>
</tr>
<tr>
<td style="text-align:left">Privileged instruction to update base/bounds</td>
<td style="text-align:left">OS must be able to set these values before letting a user program run</td>
</tr>
<tr>
<td style="text-align:left">Privileged instruction to register exception handlers</td>
<td style="text-align:left">OS must be able to tell hardware what code to run if exception occurs</td>
</tr>
<tr>
<td style="text-align:left">Ability to raise exceptions</td>
<td style="text-align:left">When processes try to access privileged instructions or out-of-bounds memory</td>
</tr>
</tbody>
</table>
<blockquote>
<p>OS support</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:left">OS Requirements</th>
<th style="text-align:left">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Memory management</td>
<td style="text-align:left">Need to allocate memory for new processes; Reclaim memory from terminated processes; Generally manage memory via free list</td>
</tr>
<tr>
<td style="text-align:left">Base/bounds management</td>
<td style="text-align:left">Must set base/bounds properly upon context switch</td>
</tr>
<tr>
<td style="text-align:left">Exception handling</td>
<td style="text-align:left">Code to run when exceptions arise; likely action is to terminate offending process</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Hardware / OS interaction</p>
</blockquote>
<img src="HardwareOS.png" alt="Hardware OS interaction" style="zoom:60%;" />
<p>Unfortunately, this simple technique of dynamic relocation does have its inefficiencies. In current approach, although there might be enough physical memory for more processes, we are currently restricted to placing an address space in a fixed-sized slot and thus <strong>internal fragmentation</strong> can arise.</p>
<h3 id="Segmentation">Segmentation</h3>
<blockquote>
<p>How do we support a large address space with a lot of free space between the stack and the heap?</p>
</blockquote>
<p>Instead of having just once base and bounds pair in our MMU, we have a pair per logical segment of the address space. What segmentation allows the OS to do is to <strong>place each one of those segments in difference parts of physical memory</strong>, and thus avoid filling physical memory with unused virtual address space.</p>
<blockquote>
<p>Hardware support</p>
</blockquote>
<p>A set of three base and bounds register pairs (segment/base/size) is required.</p>
<blockquote>
<p>Try to refer to illegal address</p>
</blockquote>
<p>The hardware detects that the address is out of bounds, traps into the OS, likely leading to the termination of the offending process. This is the origin of the famous term <strong>segment violation</strong> and <strong>segmentation fault</strong>.</p>
<blockquote>
<p>During translation, how does it know which segment an address refers</p>
</blockquote>
<p>An common and <strong>explicit</strong> approach is to <strong>chop up the address space into segments based on the top few bits of the virtual address</strong>. But using the top so many bits to select a segment is that it limits use of the virtual address space. To fully utilize the virtual address space and avoid an unused segment, some systems (only have three segments code/heap/stack) put code in the same segment as the heap and thus use only one bit to select which segment to use.</p>
<p>In the <strong>implicit</strong> approach, the hardware determines the segment by noticing how the address was formed. For example, the address was generated from the program counter, then the address is within the code segment; if the address is based off of the stack or base pointer, it must be in the stack segment; any other address must be in the heap.</p>
<blockquote>
<p>What about stack with one critical difference: it grows backwards</p>
</blockquote>
<p>We need a little extra hardware support, maybe a bit to indicate which way the segment grows.</p>
<blockquote>
<p>Support for sharing</p>
<p>To save memory, sometimes it is useful to share certain memory segments between address spaces especially code sharing.</p>
</blockquote>
<p>To support sharing, we also need a little extra hardware support in the form of <strong>protection bits</strong>. Basic support adds a few bits per segment, indicating whether or not a program can <strong>read or write</strong> a segment, or perhaps <strong>execute</strong> code that lies within the segment.</p>
<p>If a user process tries to write to a read-only segment, or execute from a non-executable segment, the hardware should <strong>raise an exception</strong>, and thus let the OS deal with the offending process.</p>
<blockquote>
<p>OS support</p>
</blockquote>
<ul>
<li>
<p>What should OS do on a context switch</p>
<p>The segment registers must be saved and restored.</p>
</li>
<li>
<p>OS interaction when segments grow or perhaps shrink</p>
<p>A program may call <code>malloc()</code> to <strong>allocate an object from existing heap</strong>.</p>
<p>When the heap segment itself need to grow, the memory-allocation library will perform a <strong>system call (<code>sbrk()</code>…) to grow the heap</strong>.</p>
<p>The OS could <strong>reject</strong> the request when no more physical memory is available or if it decides that the calling process already has too much.</p>
</li>
<li>
<p>Managing free space in physical memory</p>
<p>Allocate different sizes of physical memory for different processes and different segments makes it become full of little holes of free space, which is called <strong>external fragmentation</strong>.</p>
<p>One solution is to <strong>compact</strong> physical memory by rearranging the existing segments. However, <strong>compaction is expensive</strong>, as copying segment is memory-intensive and generally uses a fair amount of processor time.</p>
<p>A simpler approach might instead be to use a <strong>free-list management algorithm</strong> that tries to keep large extents of memory available for allocation. No matter how smart the algorithm, <strong>external fragmentation will still exist</strong>; thus, a good algorithm simply attempts to minimize it.</p>
</li>
</ul>
<blockquote>
<p>Summary</p>
</blockquote>
<p>Advantages:</p>
<p>Beyond just dynamic relocation, segmentation can better support sparse address spaces, by avoiding the huge potential waste of memory between logical segments of the address space.</p>
<p>Problems:</p>
<ul>
<li>external fragmentation is hard to avoid</li>
<li>segmentation still is <strong>not flexible enough</strong> to support our fully generalized, sparse address space.</li>
</ul>
<h3 id="Free-Space-Management">Free-Space Management</h3>
<p>It is easy when the space you are managing is divided into fixed-sized units; in such a case, you just keep a list of these fixed-sized units; when a client requests one of them, return the first entry. But when the free space you are managing consists of <strong>variable-sized units</strong>, it becomes more difficult.</p>
<blockquote>
<p>Low-level mechanisms</p>
</blockquote>
<ol>
<li>
<p>The basis of splitting and coalescing, common techniques in most any allocator</p>
<p><strong>Splitting</strong>:</p>
<p>It will find a free chunk of memory that can satisfy the request and split into two. The first chunk it will return to the caller; the second chunk will remain on the list.</p>
<p>It is commonly used in allocators when requests are smaller than the size of any particular free chunk.</p>
<p><strong>Coalesce</strong>:</p>
<p>When returning a free chunk in memory, look carefully at the address of the chunk you are returning as well as the nearby chunks of free space; if the newly-freed space sits right next to one (or two) existing free chunks, merge them into a single larger free chunk.</p>
</li>
<li>
<p>How one can track the size of allocated regions quickly and with relative ease</p>
<blockquote>
<p>The interface to <code>free(void *ptr)</code> does not take a size parameter, so when given a pointer, how the malloc library can quickly determine the size of the region of memory being freed.</p>
</blockquote>
<p>Most allocators stores a little bit of extra information in a <strong>header block</strong> which is kept in memory, usually just <strong>before the handed-out chunk of memory</strong>.</p>
<p>The header minimally contains the size of the allocated region; it may also contain additional pointers to speed up deallocation, a magic number to provide additional integrity checking, and other information.</p>
<p>Note the small but critical detail in the last sentence: the size of the free region is the size of the header plus the size of the space allocated to the user.</p>
</li>
<li>
<p>How to build a simple list inside the free space to keep track of what is free and what isn’t</p>
<p>In a more typical list, when allocating a new node, you would just call <code>malloc()</code> when you need space for the node. Instead, you need to **build the list inside the free space itself **within the memory-allocation library.</p>
<p>The description of a node of the list:</p>
<ul>
<li>node
<ul>
<li>size</li>
<li>next</li>
</ul>
</li>
</ul>
<p><code>malloc()</code>: the library allocates (request size + header size) out of the existing free chunk, returns to a pointer to it, stashes the header information immediately before the allocated space for alter use upon <code>free()</code> , and shrinks the free node in the list to (origin size - (request size + header size))</p>
<p><code>free()</code>: the library immediately figures out the size of the free region (sptr - header size), and then adds the free chunk back onto the free list.</p>
</li>
<li>
<p>What should you do if the heap runs out of space</p>
<p>The simplest approach is just to fail.</p>
<p>Request more memory from the OS by some kind of system call (e.g., <code>sbrk</code> in most unix system)</p>
</li>
</ol>
<blockquote>
<p>Basic strategies for managing free space</p>
</blockquote>
<ul>
<li>
<p>Best Fit</p>
<p>Find chunks of free memory that are as big or bigger than the requested size. Then, return the one that is the smallest in that group of candidates</p>
<p>By returning a block that is close to what the user asks, best fit tries to <strong>reduce wasted space</strong>. But naive implementations pay a <strong>heavy performance penalty</strong>.</p>
</li>
<li>
<p>Worst Fit</p>
<p>Find the largest chunk and return the requested amount; keep the remaining chunk on the free list.</p>
<p>A full search of free space is required and leading to <strong>excess fragmentation</strong> while having <strong>high overheads</strong>.</p>
</li>
<li>
<p>First Fit</p>
<p>Find the first block that is big enough and returns the requested amount to the user.</p>
<p><strong>No exhaustive search</strong> of all the free spaces are necessary but sometimes <strong>pollutes</strong> the beginning of the free list with small objects. (Solution: use address-based ordering to make coalescing become easier and fragmentation tends to be reduced)</p>
</li>
<li>
<p>Next Fit</p>
<p>Instead of always beginning the first-fit search at the beginning of the list, it keeps an extra pointer to the location within the list where one was looking last.</p>
<p>Spread the searches for free space throughout the list more <strong>uniformly</strong>, thus avoiding splintering of the beginning of the list.</p>
</li>
</ul>
<blockquote>
<p>A host of suggested techniques and algorithms to improve memory allocation in some way.</p>
</blockquote>
<ul>
<li>
<p>Segregated lists</p>
<p>If a particular application has one or a few popular-sized request that it makes, keep a separate list just to manage objects of that size; all other requests are forwarded to a more general memory allocator.</p>
<blockquote>
<p>Slab allocator</p>
</blockquote>
<p>Specifically, when the kernel boots up, it allocates a number of <strong>object caches for kernel objects</strong> that are likely to be requested frequently (such as locks, file-system inodes, etc.).</p>
<p>The object caches thus are each segregated free lists of a given size and serve memory allocation and free requests quickly. When a given cache is running low on free space, it requests some slabs of memory from <strong>a more general memory allocator</strong>.</p>
<p>Conversely, when the <strong>reference counts</strong> of the objects within a given slab all go to <strong>zero</strong>, the general allocator can <strong>reclaim</strong> them from the specialized allocator, which is often done when the VM system needs more memory.</p>
<p>By keeping free objects in a particular list in their initialized state, the slab allocator thus <strong>avoids frequent initialization and destruction cycles</strong> per objects and thus lowers overheads noticeably.</p>
</li>
<li>
<p>Buddy allocation</p>
<p>In such a system, free memory is first conceptually thought of as one big <strong>space of size 2^N</strong>. When a request for memory is made, the search for free space recursively divides free space by two until a block that is big enough to accommodate the request is found (and a further split into tow would result in space that is too small).</p>
<p>Note that this scheme can suffer from internal fragmentation, as you are only allowed to give out power-of-two-sized blocks.</p>
</li>
<li>
<p>Others</p>
<blockquote>
<p>Lack of scaling specifically searching lists can be quite slow.</p>
</blockquote>
<p>Use more complex data structures to address these costs, trading simplicity for performance. Example include balanced binary trees, splay trees or partially-ordered trees.</p>
<blockquote>
<p>Multiple processors and run multi-threaded workloads</p>
</blockquote>
</li>
</ul>
<h3 id="Paging">Paging</h3>
<p>The first approach which is to chop things up into <strong>variable-sized pieces</strong>, as we saw with <strong>segmentation</strong> in virtual memory, makes spaces become <strong>fragmented</strong>, thus allocation becomes more challenging over time.</p>
<p>The second approach is to chop up space into <strong>fixed-sized</strong> pieces. In virtual memory, we call this idea <strong>paging</strong>.</p>
<blockquote>
<p>Definition</p>
</blockquote>
<p>We divide a process’s address space into fixed-sized units, each of which we call a <strong>page</strong>.</p>
<p>We view physical memory as an array of fixed-size slots called <strong>page frames</strong>.</p>
<p>Each of these frames can contain a single virtual-memory page.</p>
<blockquote>
<p>Advantage</p>
</blockquote>
<ul>
<li>
<p>Flexibility</p>
<p>Regardless of how a process uses the address space. For example, the direction the heap and stack grow and how they are used.</p>
</li>
<li>
<p>Simplicity of free-space management</p>
</li>
</ul>
<blockquote>
<p>Address Translation</p>
</blockquote>
<p>To record where each virtual page of the address space is placed in physical memory, the operating system usually keeps a <strong>per-process</strong> data structure known as a <strong>page table</strong>.</p>
<p>To translate the virtual address that the process generated, we have to first split it into two components: the <strong>virtual page number (VPN)</strong>, and the <strong>offset</strong> within the page. With our virtual page number, we can now index our page table and find which physical frame page resides within, thus we can translate this virtual address by replacing the VPN with the <strong>PFN (physical frame number)</strong> and then issue the load to physical memory.</p>
<blockquote>
<p>Where are page tables stored</p>
</blockquote>
<p>Page tables can get terribly large, much bigger than the small segment table or base/bounds pair we have discussed previously. So we don’t keep any special on-chip hard-ware in the MMU to store the page table of the currently-running process. Instead, we store the page table for each process <strong>in memory</strong> somewhere.</p>
<blockquote>
<p>Page table organization</p>
</blockquote>
<p>The page table is just a data structure that is used to <strong>map</strong> virtual addresses to physical addresses. The simplest form is called a <strong>linear page table</strong>, which is just an array.</p>
<p>The OS indexes the array by the <strong>virtual page number (VPN)</strong>, and looks up the <strong>page-table entry (PTE)</strong> at that index in order to find the desired <strong>physical frame number (PFN)</strong>.</p>
<p><img src="pte.png" alt="pte"></p>
<p>As for the contents of each PTE, we have a number of different bits.</p>
<p>A <strong>valid bit</strong> is common to indicate whether the particular translation is valid and it is crucial for supporting a sparse address space by simply marking all the unused pages in the address space invalid.</p>
<p>The <strong>protection bits</strong> indicates whether the page could be read from, written to, or executed from.</p>
<p>A <strong>present bit</strong> § indicates whether this page is in physical memory or on disk.</p>
<p>A <strong>dirty bit</strong> (D) indicates whether the page has been modified since it was brought into memory.</p>
<p>A <strong>reference bit</strong> is sometimes used to track whether a page has been accessed, and is useful in determining which pages are popular and thus should be kept in memory.</p>
<blockquote>
<p>Problems</p>
</blockquote>
<p>Implementing paging support without care will lead to</p>
<ul>
<li>a slower machine (with many extra memory accesses to access the page table)</li>
<li>memory waste (with memory filled with page tables instead of useful application data)</li>
</ul>
<h4 id="Fast-Translation-TLBs">Fast Translation (TLBs)</h4>
<p>To speed address translation, a <strong>translation-lookaside buffer</strong> or <strong>TLB</strong> is added. A TLB is part of the chip’s <strong>memory-management unit (MMU)</strong>, and is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an <strong>address-translation</strong> cache.</p>
<p>Upon each virtual memory reference, the hardware <strong>first checks the TLB</strong> to see if the desired translation is held therein; if so, the translation is performed (quickly) without having to consult the page table (which has all translations). Because of their tremendous performance impact, TLBs in a real sense make virtual memory possible.</p>
<p>The idea behind hardware caches is to take advantage of <strong>locality</strong> in instruction and data references. There are usually two types of locality: <strong>temporal locality</strong> and <strong>spatial locality</strong>.</p>
<blockquote>
<p>TLB miss handling</p>
</blockquote>
<ul>
<li>
<p>Hardware (applied on Complex Instruction Sets)</p>
<p>On a miss, the hardware would “walk” the page table, find the correct page-table entry and extract the desired translation, update the TLB with translation, and retry the instruction.</p>
</li>
<li>
<p>OS (applied on Reduced Instruction Sets) (flexibility and simplicity)</p>
<p>On a miss, the hardware simply raises an exception, which pause s the current instruction stream, raises the privilege level to kernel mode, and jumps to a <strong>trap handler</strong>.</p>
</li>
</ul>
<p>The return-from-trap instruction needs to be a little different than the return-from-trap we saw before when servicing a system call.</p>
<p>In the latter case, the return-from-trap should resume execution at the instruction after the trap after the trap into the OS, just as a return from a procedure call returns to the instruction immediately following the call into the procedure.</p>
<p>In the former case when returning from a TLB miss-handling trap, the hardware must <strong>resume execution at the instruction that caused the trap</strong>; this retry thus lets the instruction run again, this time resulting in a TLB bit.</p>
<blockquote>
<p>Context Switches</p>
<p>To correctly and efficiently support virtualization across multiple processes.</p>
</blockquote>
<ul>
<li>
<p>Simply <strong>flush</strong> the TLB on context switches</p>
<p>Overhead: each time a process runs, it must incur TLB misses as it touches its data and code pages.</p>
</li>
<li>
<p>Provide an <strong>address space identifier</strong> field in the hardware TLB</p>
<p>With address-space identifiers, the TLB can hold translations from different processes at the same time without any confusion.</p>
</li>
</ul>
<blockquote>
<p>Replacement policy</p>
</blockquote>
<p>We’ll talk about that later.</p>
<h4 id="Smaller-Tables">Smaller Tables</h4>
<blockquote>
<p>With a hundred active processes, we will be allocating hundreds of megabytes of memory just for page tables, so we need to find some techniques to reduce the heavy burden.</p>
</blockquote>
<ul>
<li>
<p>Bigger pages</p>
<p>Disadvantage: big pages leads to waste within each page (<strong>Internal fragmentation</strong>)</p>
</li>
<li>
<p>Paging and segments (Hybrid)</p>
<p>Instead pf having a single page table for the entire address space of the process, it have one per logical segment. Use base register to hold the <strong>physical address of the page table</strong> of that segment, the bound register to indicate the end of the page table.</p>
<p>Disadvantage: segmentation is not so flexible and cause <strong>external fragmentation</strong>.</p>
</li>
<li>
<p>Multi-level page tables</p>
<p>Chop up the page table into page-sized units, if an entire page of page-table entries is invalid, don’t allocate that page of the page table at all. Use a new structure, <strong>page directory</strong> to trace where a page of the page table is, or the entire page of the page table contains no valid pages.</p>
<p>On a TLB miss, two loads from memory will be required to get the right translation information from the page table (one for the page directory, and one for the PTE itself). It is a example of a <strong>time-space trade-off</strong></p>
</li>
</ul>
<h3 id="Beyond-Physical-Memory">Beyond Physical Memory</h3>
<blockquote>
<p>To support large address spaces, the OS will need a place to stash away portions of address spaces that currently aren’t in great demand.</p>
</blockquote>
<p>Why a single large address space for a process is needed? (Convenience and ease of use)</p>
<ul>
<li>Don’t have to worry about if there is room enough in memory for your data structures</li>
<li>Beyond just a single process, the addition of swap space allows the OS to support the illusion of a large virtual memory for multiple concurrently-running progress.</li>
</ul>
<blockquote>
<p>Page Fault</p>
</blockquote>
<p>In either type of system (hardware-managed or software-managed), if a page is not present, the OS is put in charge to handle the page fault. The appropriately-named OS <strong>page-fault handler</strong> runs to determine what to do.</p>
<p>Why hardware-managed TLBs system doesn’t handle page faults?</p>
<ol>
<li>Page faults to disk are slow and the extra overheads of running software are minimal.</li>
<li>To handle a page fault, the hardware would have to understand swap space, how to issue I/Os to the disk, and a lot of other details which it currently doesn’t know much about.</li>
</ol>
<blockquote>
<p>What if memory is full</p>
</blockquote>
<p>We only discussed the <strong>page in</strong> case above, but in fact the memory may be full, thus the OS might like to first <strong>page out</strong> one or more pages to make room for the new page the OS is about to bring in. The process of picking a page to kick out, or replace is known as the <strong>page-replacement policy</strong>.</p>
<blockquote>
<p>Thrashing</p>
<p>What should the OS do when memory is simply oversubscribed, and the memory demands of the set of running processes simply exceeds the available physical memory?</p>
</blockquote>
<ul>
<li>
<p>Admission control</p>
<p>Given a set of processes, a system could decide not to run a subset of processes, with the hope that the reduced set of processes’ working sets fit in memory and thus can make progress.</p>
</li>
<li>
<p>Out-of-memory killer</p>
<p>The daemon choose a memory-intensive process and kills it, thus reducing memory in a non-too-subtle manner.</p>
</li>
</ul>
<blockquote>
<p>Type of cache misses</p>
<p>In the computer architecture word, architects sometimes find it useful to characterize misses by type, into one of three categories:</p>
</blockquote>
<ul>
<li>
<p>Compulsory miss (cold-start miss)</p>
<p>The cache is empty to begin with and this is the first reference to the item</p>
</li>
<li>
<p>Capacity miss</p>
<p>the cache ran out of space and had to evict an item to bring a new item into the cache.</p>
</li>
<li>
<p>Conflict miss</p>
<p>Arises in hardware because of limits on where an item can be placed in a hardware cache, due to something known as <strong>set-associativity</strong>. It does <strong>not</strong> arises in the OS page cache because such caches are always <strong>fully-associative</strong></p>
</li>
</ul>
<h4 id="Replacement-Policy">Replacement Policy</h4>
<ol>
<li>
<p>FIFO</p>
<blockquote>
<p>Belady 's anomaly</p>
<p>In general, you would expect the cache hit rate to increase when the cache gets larger, but with some replacement policies such as FIFO, it gets worse.</p>
</blockquote>
</li>
<li>
<p>Random</p>
</li>
<li>
<p>LRU</p>
<p>Lean on the past and use history as our guide to improve our guess at the future.</p>
<p>Historical information:</p>
<ul>
<li>Frequency</li>
<li>Recency</li>
</ul>
<p>To keep track of which pages have least and most recently used, the system has to do some <strong>accounting work</strong> on every memory reference, which could greatly reduce performance.</p>
</li>
<li>
<p>Approximating LRU</p>
<p>We don’t really find the absolute oldest page to replace and just survive with approximation. The idea requires some hardware support, in the form if a <strong>use bit</strong>.</p>
<ul>
<li>
<p>Clock algorithm</p>
</li>
<li>
<p>Small modification to the clock algorithm</p>
<p>Use <strong>modified (dirty) bit</strong> to indicate whether a page has been modified or not while in memory. If a page has been modified, it must be written back to disk to evict it and it is expensive.</p>
</li>
</ul>
</li>
<li>
<p>Other VM Policies</p>
<ul>
<li>
<p>Demand paging (<strong>When</strong> to bring a page into memory)</p>
<p>The OS brings the page into memory when it is accessed.</p>
</li>
<li>
<p>Clustering / grouping of writes (<strong>How</strong> the OS writes pages out to disk)</p>
<p>The OS collect a number of pending writes together in memory and write them to disk in one write.</p>
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Operating System</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ Smart Pointers</title>
    <url>/2020/08/12/pointer/</url>
    <content><![CDATA[<h1>C++ Smart Pointers</h1>
<p>与托管语言不同，C没有自动垃圾回收功能（在程序运行时释放堆内存和其他资源）。C程序负责将获取的所有资源返回到操作系统，己动态分配的堆内存由于某种原因程序未释放或无法释放，造成系统内存的浪费被称为<strong>内存泄漏</strong>。在进程退出之前，泄漏的资源对其他程序不可用。总而言之，内存泄漏是 C-style编程中出现 bug 的一个常见原因。</p>
<h3 id="RAII">RAII</h3>
<p>现代c++通过在堆栈上声明对象来尽可能避免使用堆内存。当一个资源对于堆栈来说太大时，它应该被一个对象所拥有。当对象被初始化时，它获得它所拥有的资源。然后，该对象负责释放其析构函数中的资源。拥有的对象本身是在堆栈上声明的。对象拥有资源的原则也称为 “<strong>Resource Acquisition Is Initialization</strong>” 或 <strong>RAII</strong>。</p>
<p>当拥有资源的堆栈对象超出作用域时，将自动调用其析构函数。这样，C ++中的垃圾回收与对象生存期密切相关，并且是确定性的。资源总是在程序的可控的已知位置释放。这利用了核心语言特性(对象生存期、范围退出、初始化顺序和堆栈展开)来消除资源泄漏并保证异常安全性。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">std::mutex m;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bad</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    m.<span class="built_in">lock</span>();                    <span class="comment">// acquire the mutex</span></span><br><span class="line">    <span class="built_in">f</span>();                         <span class="comment">// if f() throws an exception, the mutex is never released</span></span><br><span class="line">    <span class="keyword">if</span>(!<span class="built_in">everything_ok</span>()) <span class="keyword">return</span>; <span class="comment">// early return, the mutex is never released</span></span><br><span class="line">    m.<span class="built_in">unlock</span>();                  <span class="comment">// if bad() reaches this statement, the mutex is released</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">good</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">std::lock_guard&lt;std::mutex&gt; <span class="title">lk</span><span class="params">(m)</span></span>; <span class="comment">// RAII class: mutex acquisition is initialization</span></span><br><span class="line">    <span class="built_in">f</span>();                               <span class="comment">// if f() throws an exception, the mutex is released</span></span><br><span class="line">    <span class="keyword">if</span>(!<span class="built_in">everything_ok</span>()) <span class="keyword">return</span>;       <span class="comment">// early return, the mutex is released</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过使用智能指针进行内存分配，可以消除内存泄漏的可能性。它们的行为类似常规指针，重要的区别是它负责自动释放所指向的对象。<code>shared_ptr</code> 和 <code>unique_ptr</code> 的区别在于管理底层指针的方式：<code>shared_ptr</code> 允许多个指针指向同一个对象；<code>unique_ptr</code> 则“独占”所指向的对象。<code>weak_ptr</code> 是一种弱引用，指向 <code>shared_ptr</code> 所管理的对象。</p>
<p>智能指针是<strong>模板</strong>，当创建时，需要提供指针指向的类型。默认初始化的智能指针保存着一个空指针。</p>
<h3 id="unique-ptr">unique_ptr</h3>
<p>只允许底层指针有一个所有者，与原始指针一样高效。<code>unique_ptr</code> 的基本特征：大小是<strong>一个指针</strong>，可移动（将所有权转移到新 <code>unique_ptr</code> 并重置旧 <code>unique_ptr</code>），但<strong>不可复制或共享</strong>。<code>unique_ptr</code> 通过引用来传递，如果尝试通过此处的值传递，由于删除了 <code>unique_ptr</code> 复制构造函数，编译器将引发错误。</p>
<p><img src="unique_ptr.png" alt="unique_ptr"></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//Example</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Use make_unique function.(C++14)</span></span><br><span class="line">    <span class="keyword">auto</span> p1 = <span class="built_in">make_unique</span>&lt;MyClass&gt;(<span class="string">&quot;argv&quot;</span>);</span><br><span class="line">    <span class="comment">// Move raw pointer from one unique_ptr to another.</span></span><br><span class="line">    unique_ptr&lt;MyClass&gt; p2 = std::<span class="built_in">move</span>(p1);</span><br><span class="line">    <span class="comment">// Create a unique_ptr to an array of 5 integers.</span></span><br><span class="line">    <span class="keyword">auto</span> p3 = <span class="built_in">make_unique</span>&lt;<span class="type">int</span>[]&gt;(<span class="number">5</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="shared-ptr">shared_ptr</h3>
<p>采用引用计数的智能指针，专门用于可能需要多个所有者管理内存中对象的生命周期的方案。<strong>大小为两个指针</strong>；一个用于<strong>对象</strong>，另一个用于包指向<strong>共享控制块</strong>。</p>
<p>控制块是一个动态分配的对象，其中包含：</p>
<ul>
<li>指向被管理对象的指针或被管理对象本身</li>
<li>删除器（类型擦除）</li>
<li>分配器（类型擦除）</li>
<li>占有被管理对象的 <code>shared_ptr</code> 的数量</li>
<li>涉及被管理对象的 <code>weak_ptr</code> 的数量</li>
</ul>
<p>初始化 <code>shared_ptr</code> 之后，您可以复制它，在函数参数中按值传递它，并将其分配给其他 <code>shared_ptr</code> 实例。 所有实例都指向同一个对象，并且共享对一个控制块的访问权，每当添加新的 <code>shared_ptr</code>，超出范围或对其进行重置时，该控制块都会递增和递减<code>shared_ptr</code> 引用计数。如果<code>shared_ptr</code> 引用计数减至零，控制块就会调用被管理对象的析构函数。但控制块本身直到 <code>weak_ptr</code> 计数器同样归零时才会释放。</p>
<p><code>shared_ptr</code> 的默认内存布局（通过构造函数）</p>
<img src="shared_ptr1.png" alt="shared_ptr" style="zoom:45%;" />
<p>使用<code>make_shared</code> / <code>allocate_shared</code>创建时的内存布局：</p>
<img src="shared_ptr.png" alt="shared_ptr" style="zoom:50%;" />
<blockquote>
<p><a href="https://stackoverflow.com/questions/34046070/why-are-two-raw-pointers-to-the-managed-object-needed-in-stdshared-ptr-impleme">Why are two raw pointers to the managed object needed in std::shared_ptr implementation?</a></p>
</blockquote>
<p>首次创建内存资源时，尽可能使用 <code>make_shared</code> 函数创建 <code>shared_ptr</code>。 <code>make_shared</code> 是 exception-safe 的。它在同一个调用中为控制块和资源分配内存 (<a href="https://stackoverflow.com/questions/20895648/difference-in-make-shared-and-normal-shared-ptr-in-c">Difference in make_shared and normal shared_ptr in C++</a>)，从而减少了构造开销。 如果不使用 <code>make_shared</code>，则必须先使用显式新表达式创建对象，然后再将其传递给<code>shared_ptr</code>构造函数。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//Example</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Use make_shared function when possible.</span></span><br><span class="line">    <span class="keyword">auto</span> sp1 = <span class="built_in">make_shared</span>&lt;MyClass&gt;(<span class="string">&quot;argv&quot;</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Ok, but slightly less efficient. </span></span><br><span class="line">    <span class="function">shared_ptr&lt;MyClass&gt; <span class="title">sp2</span><span class="params">(<span class="keyword">new</span> MyClass(<span class="string">&quot;argv&quot;</span>))</span></span>;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">//Initialize with copy constructor. Increments ref count.</span></span><br><span class="line">	<span class="function"><span class="keyword">auto</span> <span class="title">sp3</span><span class="params">(sp2)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//Initialize via assignment. Increments ref count.</span></span><br><span class="line">	<span class="keyword">auto</span> sp4 = sp2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="weak-ptr">weak_ptr</h3>
<p>结合 <code>shared_ptr</code> 使用的特例智能指针，不控制所指向对象生命周期的智能指针。<strong>大小为两个指针</strong>；一个用于存储构建它的 <code>shared_ptr</code> 的指针，另一个用于指向 <strong>共享控制块</strong>。将一个 <code>weak_ptr</code> 绑定到一个 <code>shared_ptr</code> 不会改变 <code>shared_ptr</code> 的引用计数，一旦最后一个指向对象的 <code>shared_ptr</code> 被销毁，对象就会被释放。但控制块本身直到 <code>weak_ptr</code> 计数器同样归零时才会释放。</p>
<p>当<code>weak_ptr</code>需要获得临时所有权时，需要将其转换为 <code>shared_ptr</code>，此时如果原来的 <code>shared_ptr</code> 被销毁，则该对象的生命期将被延长至这个临时的 <code>shared_ptr</code> 同样被销毁为止。</p>
<p>最好的设计是尽可能<strong>避免共享</strong>指针的所有权。但是，如果必须共享 <code>shared_ptr</code> 实例的所有权，则应避免在它们之间循环引用。</p>
<h4 id="circular-references">circular references</h4>
<p>在基于引用计数的任何类型的系统中，对类的引用可以形成循环，即第一个对象引用第二个对象，第二个对象引用第三个对象，依此类推，直到某个最终对象引用回第一个对象。因此，引用计数永远不会达到0，对象也永远不会被释放。</p>
<img src="cyclic.png" alt="cyclic reference" style="zoom:80%;" />
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//Example</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    shared_ptr&lt;B&gt; b_ptr;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:    </span><br><span class="line">    shared_ptr&lt;A&gt; a_ptr;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> obj1 = <span class="built_in">make_shared</span>&lt;A&gt;(<span class="keyword">new</span> <span class="built_in">A</span>());</span><br><span class="line">    <span class="keyword">auto</span> obj2 = <span class="built_in">make_shared</span>&lt;B&gt;(<span class="keyword">new</span> <span class="built_in">B</span>());</span><br><span class="line">    obj1-&gt;b_ptr = obj2;</span><br><span class="line">    obj2-&gt;a_ptr = obj1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当 <code>shared_ptr</code> 离开作用域时，引用计数仍然都是1，因此A和B对象不会被删除。</p>
<img src="cyclic_solution.png" alt="solution" style="zoom:67%;" />
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//Solution</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    shared_ptr&lt;B&gt; b_ptr;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:    </span><br><span class="line">    weak_ptr&lt;A&gt; a_ptr;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">useA</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">auto</span> a = a_ptr.<span class="built_in">lock</span>();</span><br><span class="line">        <span class="keyword">if</span>(a) &#123;	<span class="comment">//<span class="doctag">TODO:</span> do something to a &#125;</span></span><br><span class="line">        <span class="keyword">else</span> &#123; <span class="comment">//<span class="doctag">TODO:</span> a has been deleted &#125;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> obj1 = <span class="built_in">make_shared</span>&lt;A&gt;(<span class="keyword">new</span> <span class="built_in">A</span>());</span><br><span class="line">    <span class="keyword">auto</span> obj2 = <span class="built_in">make_shared</span>&lt;B&gt;(<span class="keyword">new</span> <span class="built_in">B</span>());</span><br><span class="line">    obj1-&gt;b_ptr = obj2;</span><br><span class="line">    obj2-&gt;a_ptr = obj1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="References">References</h3>
<p><a href="https://docs.microsoft.com/en-us/cpp/cpp/smart-pointers-modern-cpp?view=vs-2019">Smart pointers (Modern C++) | Microsoft Docs</a></p>
<p><a href="https://en.cppreference.com/w/cpp/memory">Dynamic memory management | cppreference.com</a></p>
<p><a href="https://docs.huihoo.com/actor-framework/en/ReferenceCounting.html">Reference Counting | CAF</a></p>
]]></content>
      <categories>
        <category>Concepts</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ Rvalue Reference</title>
    <url>/2020/07/01/rvalue/</url>
    <content><![CDATA[<h1>C++ Rvalue Reference</h1>
<h2 id="Value-categories">Value categories</h2>
<p><img src="value_categories.png" alt="Value-categories"></p>
<p>每个 C++ 表达式（带有操作数的操作符、字面量、变量名等）都属于三种基本值类别中的一种：</p>
<p>更具体的示例描述请查看 <a href="https://en.cppreference.com/w/cpp/language/value_category">Value categories – cppreference.com</a></p>
<ul>
<li>
<p>纯右值 (prvalue)</p>
<p>纯右值表达式没有可通过程序访问的地址。</p>
<p>纯右值表达式的示例包括：（除了字符串字面量之外的）字面量、返回非引用类型的函数调用，以及在表达式评估期间创建但仅由编译器访问的临时对象</p>
</li>
<li>
<p>亡值 (xvalue)</p>
<p>亡值表达式的地址不能再由您的程序访问，但可用于初始化 rvalue 引用，后者提供对表达式的访问。</p>
<p>亡值表达式的示例包括返回右值引用的函数调用，以及数组下标、成员和指针到数组或对象为右值引用的成员表达式</p>
</li>
<li>
<p>左值 (lvalue)</p>
<p>左值具有程序可以访问的地址。</p>
<p>左值表达式的示例包括变量名称，包括<strong>常量</strong>变量、数组元素、返回 lvalue 引用、位字段、联合和类成员的函数调用</p>
</li>
</ul>
<p>随着移动语义引入到 C++11 之中，值类别被重新进行了定义，以区别表达式的两种独立的性质：</p>
<ul>
<li><em>拥有身份 ( has identity)</em>：可以确定表达式是否与另一表达式指代同一实体，例如通过比较它们所标识的对象或函数的（直接或间接获得的）地址；</li>
<li><em>可被移动</em>：移动构造函数、移动赋值运算符或实现了移动语义的其他函数重载能够绑定于这个表达式。</li>
</ul>
<p>C++11 中：</p>
<ul>
<li>拥有身份且不可被移动的表达式被称作*左值 (lvalue)*表达式；</li>
<li>拥有身份且可被移动的表达式被称作*亡值 (xvalue)*表达式；</li>
<li>不拥有身份且可被移动的表达式被称作*纯右值 (prvalue)*表达式；</li>
<li>不拥有身份且不可被移动的表达式无法使用。</li>
</ul>
<p>拥有身份的表达式被称作“泛左值 (glvalue) 表达式”。左值和亡值都是泛左值表达式。</p>
<p>可被移动的表达式被称作“右值 (rvalue) 表达式”。纯右值和亡值都是右值表达式。</p>
<p><img src=".value-categories.png" alt="value-categories"></p>
<h2 id="Lvalue-Reference">Lvalue Reference</h2>
<p>c++11之前，C 中仅存在一种引用类型，所以简单称之为<em>引用</em>，而在c11中其被称为左值引用</p>
<table>
<thead>
<tr>
<th style="text-align:left">L-value reference</th>
<th style="text-align:left">Can be initialized with</th>
<th style="text-align:left">Can modify</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Modifiable l-values</td>
<td style="text-align:left">Yes</td>
<td style="text-align:left">Yes</td>
</tr>
<tr>
<td style="text-align:left">Non-modifiable l-values</td>
<td style="text-align:left">No</td>
<td style="text-align:left">No</td>
</tr>
<tr>
<td style="text-align:left">R-values</td>
<td style="text-align:left">No</td>
<td style="text-align:left">No</td>
</tr>
</tbody>
</table>
<p>而对const对象的左值引用可以使用左值或右值进行初始化</p>
<table>
<thead>
<tr>
<th style="text-align:left">L-value reference to const</th>
<th style="text-align:left">Can be initialized with</th>
<th style="text-align:left">Can modify</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Modifiable l-values</td>
<td style="text-align:left">Yes</td>
<td style="text-align:left">No</td>
</tr>
<tr>
<td style="text-align:left">Non-modifiable l-values</td>
<td style="text-align:left">Yes</td>
<td style="text-align:left">No</td>
</tr>
<tr>
<td style="text-align:left">R-values</td>
<td style="text-align:left">Yes</td>
<td style="text-align:left">No</td>
</tr>
</tbody>
</table>
<p>对const对象的左值引用非常有用，因为它使得可以将任何类型的参数（左值或右值）传递给函数，而无需复制该参数</p>
<h2 id="Rvalue-Reference">Rvalue Reference</h2>
<p>右值引用可将左值和右值区分开，可以帮助您不必要的内存分配和复制操作需求，从而提高应用程序的性能。</p>
<p>相比于左值引用，</p>
<table>
<thead>
<tr>
<th style="text-align:left">R-value reference</th>
<th style="text-align:left">Can be initialized with</th>
<th style="text-align:left">Can modify</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Modifiable l-values</td>
<td style="text-align:left">No</td>
<td style="text-align:left">No</td>
</tr>
<tr>
<td style="text-align:left">Non-modifiable l-values</td>
<td style="text-align:left">No</td>
<td style="text-align:left">No</td>
</tr>
<tr>
<td style="text-align:left">R-values</td>
<td style="text-align:left">Yes</td>
<td style="text-align:left">Yes</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:left">R-value reference to const</th>
<th style="text-align:left">Can be initialized with</th>
<th style="text-align:left">Can modify</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Modifiable l-values</td>
<td style="text-align:left">No</td>
<td style="text-align:left">No</td>
</tr>
<tr>
<td style="text-align:left">Non-modifiable l-values</td>
<td style="text-align:left">No</td>
<td style="text-align:left">No</td>
</tr>
<tr>
<td style="text-align:left">R-values</td>
<td style="text-align:left">Yes</td>
<td style="text-align:left">No</td>
</tr>
</tbody>
</table>
<h2 id="Move-Semantics">Move Semantics</h2>
<p>要实现移动语义，通常向类提供<em>移动构造函数，</em> 并可以选择移动赋值运算符 （<strong>运算符 =</strong>）。 其源是右值的复制和赋值操作随后会自动利用移动语义。 与默认复制构造函数不同，编译器不提供默认移动构造函数。</p>
<p>假设一个类X，其中包含一个/多个指向 创建和拷贝开销较大的对象(资源) 的指针，则无需取消引用该指针并复制其内容，移动构造（拷贝）函数只需复制指针的地址， 这就是移动构造函数带来的性能优势所在（只需要浅拷贝）。</p>
<p>（由于参数是非常量引用，因此它可以修改传递给它的对象，可能具有破坏性）</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">X&amp; X::<span class="keyword">operator</span>=(<span class="type">const</span> X &amp; rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//make a clone of what rhs&#x27;s member pointer refers to</span></span><br><span class="line">    <span class="comment">//destruct the resources that this member pointer refers to</span></span><br><span class="line">    <span class="comment">//attach the clone</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">X&amp; X::<span class="keyword">operator</span>=(X &amp;&amp; rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//swap this-&gt;pointer and rhs.pointer</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="built_in">main</span>()</span><br><span class="line">&#123;</span><br><span class="line">    X a;</span><br><span class="line">    a = <span class="built_in">X</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因此，通过const lvalue 引用或 rvalue 引用参数重载函数，来编写区分不可修改对象 （lvalue） 和可修改临时值 （rvalue） 的代码。</p>
<h3 id="Forcing-Move-Semantics">Forcing Move Semantics</h3>
<p>如果你想通过一个变量(lvalue)来调用移动构造（拷贝）函数，std::move()会将变量(lvalue)转换为右值，使其绑定到移动构造(拷贝)函数</p>
<blockquote>
<p>std::move() does not move, it casts.</p>
<p>std::move()实际上是通过static_cast实现的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">return static_cast&lt;typename std::remove_reference&lt;T&gt;::type&amp;&amp;&gt;(t);</span><br></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt; </span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">swap</span><span class="params">(T&amp; a, T&amp; b)</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">  <span class="function">T <span class="title">tmp</span><span class="params">(std::move(a))</span></span>;</span><br><span class="line">  a = std::<span class="built_in">move</span>(b); </span><br><span class="line">  b = std::<span class="built_in">move</span>(tmp);</span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    X a, b;</span><br><span class="line">	<span class="built_in">swap</span>(a, b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因为已经将变量强制转换为右值，而接收右值的函数可能会对该变量产生破坏性作用，在此之后使用变量的内容可能会导致不确定的行为，因此在变量经过std::move()之后，需要销毁或重新分配。</p>
<h2 id="Perfect-forwarding">Perfect forwarding</h2>
<p>完美转发可减少对重载函数的需求，并有助于避免转发问题。 当编写一个泛型函数，该函数将<em>引用</em>作为其参数，并将这些参数传递给（或<em>转发</em>）到另一个函数时，可能会出现<em>转发问题</em>。 例如，如果泛型函数采用 <code>const T&amp;</code> 类型的参数，则调用的函数无法修改该参数的值。 如果泛型函数采用 <code>T&amp;</code> 类型的参数，则无法使用右值（如临时对象或非字符串字面量）来调用该函数。</p>
<p>通常，若要解决此问题，则必须提供为其每个参数采用 <code>T&amp;</code> 和 <code>const T&amp;</code> 的重载版本的泛型函数。 因此，重载函数的数量将基于参数的数量呈指数方式增加。 利用右值引用，函数模板会推导出其模板自变量类型，然后使用引用折叠规则，函数可接受任意参数并将其转发给另一个函数，就像已直接调用其他函数一样。</p>
<p>引用折叠规则：</p>
<ul>
<li>T&amp; &amp; -&gt; T&amp;</li>
<li>T&amp; &amp;&amp; -&gt; T&amp;</li>
<li>T&amp;&amp; &amp; -&gt; T&amp;</li>
<li>T&amp;&amp; &amp;&amp; -&gt; T&amp;&amp;</li>
</ul>
<p>使用示例:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> Arg&gt; </span></span><br><span class="line"><span class="function">shared_ptr&lt;T&gt; <span class="title">factory</span><span class="params">(Arg&amp;&amp; arg)</span></span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">shared_ptr</span>&lt;T&gt;(<span class="keyword">new</span> <span class="built_in">T</span>(std::forward&lt;Arg&gt;(arg)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>std::forward从概念上所实现：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (is_lvalue_reference&lt;T&gt;::value)<span class="comment">//Code producing an lvalue reference expects the object to remain valid.</span></span><br><span class="line">  <span class="keyword">return</span> t;</span><br><span class="line"><span class="keyword">return</span> std::<span class="built_in">move</span>(t);</span><br></pre></td></tr></table></figure>
<h2 id="References">References</h2>
<p><a href="https://docs.microsoft.com/en-us/cpp/cpp/rvalue-reference-declarator-amp-amp?view=vs-2019#syntax">Rvalue Reference Declarator: &amp;&amp; - Microsoft Docs</a></p>
<p><a href="https://docs.microsoft.com/en-us/cpp/cpp/lvalues-and-rvalues-visual-cpp?view=vs-2015">Value Categories - Microsoft Docs</a></p>
<p><a href="https://en.cppreference.com/w/cpp/language/value_category">Value categories - cppreference.com</a></p>
<p><a href="https://www.chromium.org/rvalue-references?tmpl=%2Fsystem%2Fapp%2Ftemplates%2Fprint%2F&amp;showPrintDialog=1#TOC-1.-What-makes-rvalues.">Rvalue references in Chromium</a></p>
]]></content>
      <categories>
        <category>Concepts</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
      </tags>
  </entry>
</search>
